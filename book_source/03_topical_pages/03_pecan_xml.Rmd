# The PEcAn XML {#pecanXML}

The PEcAn system is configured using a XML file, often called `pecan.xml`.
It contains the following major sections ("nodes"):

- [Core configuration](#xml-core-config)
	- [Top level structure](#xml-structure)
	- [`info`](#xml-info) -- Run metadata
	- [`outdir`](#xml-outdir) -- Output directory
	- [`database`](#xml-database) -- PEcAn database settings
	- [`pft`](#xml-pft) -- Plant functional type selection
	- [`meta.analysis`](#xml-meta-analysis) -- Trait meta analysis
	- [`model`](#xml-model) -- Model configuration
	- [`run`](#xml-run) -- Run setup
	- [`host`](#xml-host) -- Host information for remote execution
- [Advanced features](#xml-advanced)
	- [`ensemble`](#xml-ensemble) -- Ensemble runs
	- [`sensitivity.analysis`](#xml-sensitivity-analysis) -- Sensitivity analysis
	- [`parameter.data.assimilation`](#xml-parameter-data-assimilation) -- Parameter data assimilation
	- [`multi.settings`](#xml-multi-settings) -- Multi Site Settings
	- (experimental) [`state.data.assimilation`](#xml-state-data-assimilation) -- State data assimilation
	- (experimental) [`benchmarking`](#xml-benchmarking) -- Benchmarking
	- [`remote_process`](#xml-remote_process) -- Remote data module 
	
A basic example looks like this:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<pecan>
  <info>
    <notes>Example run</notes>
    <userid>-1</userid>
	<username>guestuser</username>
    <date>2018/09/18 19:12:28 +0000</date>
  </info>
  <outdir>/data/workflows/PEcAn_99000000006</outdir>
  <database>
    <bety>
      <user>bety</user>
      <password>bety</password>
      <host>postgres</host>
      <dbname>bety</dbname>
      <driver>PostgreSQL</driver>
      <write>true</write>
    </bety>
    <dbfiles>/data/dbfiles</dbfiles>
  </database>
  <pfts>
    <pft>
      <name>tundra.grasses</name> 
      <constants>
        <num>1</num>
      </constants>
    </pft>
  </pfts>
  <meta.analysis>
    <iter>3000</iter>
    <random.effects>
     <on>FALSE</on>
     <use_ghs>TRUE</use_ghs>
    </random.effects>
  </meta.analysis>
  <ensemble>
   <size>1</size>
   <variable>NPP</variable>
   <samplingspace>
   <parameters>
    <method>uniform</method>
   </parameters>
   <met>
    <method>sampling</method>
 	</met>
   </samplingspace>
  </ensemble>
  <model>
    <id>5000000002</id>
  </model>
  <workflow>
    <id>99000000006</id>
  </workflow>
  <run>
    <site>
      <id>1000000098</id>
      <met.start>2004/01/01</met.start>
      <met.end>2004/12/31</met.end>
    </site>
    <inputs>
      <met>
        <source>CRUNCEP</source>
        <output>SIPNET</output>
      </met>
    </inputs>
    <start.date>2004/01/01</start.date>
    <end.date>2004/12/31</end.date>
  </run>
  <host>
    <name>localhost</name>
    <rabbitmq>
      <uri>amqp://guest:guest@rabbitmq:5672/%2F</uri>
      <queue>SIPNET_136</queue>
    </rabbitmq>
  </host>
</pecan>
```

In the following sections, we step through each of these sections in detail.

## Core configuration {#xml-core-config}

### Top-level structure {#xml-structure}

The first line of the XML file should contain version and encoding information. 

```xml
<?xml version="1.0" encoding="UTF-8"?>
```

The rest of the XML file should be surrounded by `<pecan>...</pecan>` tags.

```xml
<pecan>
  ...XML body here...
</pecan>
```

### `info`: Run metadata {#xml-info}

This section contains run metadata.
This information is not essential to a successful model run, but is useful for tracking run provenance. 

```xml
  <info>
    <notes>Example run</notes>
    <userid>-1</userid>
    <username>guestuser</username>
    <date>2018/09/18 19:12:28 +0000</date>
  </info>
```

The `<notes>` tag will be filled in by the web GUI if you provide notes, or you can add notes yourself within these tags. We suggest adding notes that help identify your run and a brief description of what the run is for. Because these notes are searchable within the PEcAn database and web interface, they can be a useful way to distinguish between similar runs.

The `<userid>` and `<username>` section is filled in from the GUI if you are signed in. If you are not using the GUI, add the user name and ID you are associated with that exists within the PEcAn database.

The `<date></date>` tag is filled automatically at the time of your run from the GUI. If you are not using the GUI, add the date you execute the run. This tag is not the tag for the dates you would like to run your model simulation.

### `outdir`: Output directory {#xml-outdir}

The `<outdir>` tag is used to configure the output folder used by PEcAn.
This is the directory where all model input and output files will be stored.
By default, the web interface names this folder `PEcAn_<workflow ID>`, and higher-level location is set by the `$output_folder$` variable in the `web/config.php` file.
If no `outdir` is specified, PEcAn defaults to the working directory from which it is called, which may be counterintuitive.

```xml
  <outdir>/data/workflows/PEcAn_99000000006</outdir>
```

### `database`: PEcAn database settings {#xml-database}

#### `bety`: PEcAn database (Bety) configuration {#xml-bety}

The `bety` tag defines the driver to use to connect to the database (we support `PostgreSQL`, which is the default, and `Postgres`) and parameters required to connect to the database. Note that connection parameters are passed *exactly* as entered to the underlying R database driver, and any invalid or extra parameters will result in an error.

In other words, this configuration...

```xml
  <database>
	...
    <bety>
      <user>bety</user>
      <password>bety</password>
      <host>postgres</host>
      <dbname>bety</dbname>
      <driver>PostgreSQL</driver>
      <port>5432</port>
      <write>true</write>
    </bety>
	...
  </database>
```

...will be translated (by way of `PEcAn.DB::db.open(settings$database$bety)`) into R code like the following:

```r
con <- DBI::dbConnect(
  drv = RPostgreSQL::PostgreSQL(),
  user = "bety",
  password = "bety",
  dbname = "bety",
  host = "postgres",
  port = "5432",
  write = TRUE
)
```

Common parameters are described as follows:

* `driver`: The driver to use to connect to the database. This should always be set to `PostgreSQL`, unless you absolutely know what you're doing.
* `dbname`: The name of the database (formerly `name`), corresponding to the `-d` argument to `psql`. In most cases, this should be set to `bety`, and will only be different if you named your Bety instance something else (e.g. if you have multiple instances running at once). If unset, it will default to the user name of the current user, which is usually wrong!
* `user`: The username to connect to the database (formerly `userid`), corresponding to the `-U` argument to `psql`. default value is the username of the current user logged in (PostgreSQL uses user for this field).
* `password`: The password to connect to the database (was `passwd`), corresponding to the `-p` argument to `psql`. If unspecified, no password is used. On standard PEcAn installations, the username and password are both `bety` (all lowercase).
* `host`: The hostname of the `bety` database, corresponding to the `-h` argument to `psql`. On the VM, this will be `localhost` (the default). If using `docker`, this will be the name of the PostgreSQL container, which is `postgres` if using our standard `docker-compose`. If connecting to the PEcAn database on a remote server (e.g. `psql-pecan.bu.edu`), this should be the same as the hostname used for `ssh` access.
* `write`: Logical. If `true` (the default), write results to the database. If `false`, PEcAn will run but will not store any information to `bety`.

When using the web interface, this section is configured by the `web/config.php` file.
The default `config.php` settings on any given platform (VM, Docker, etc.) or in example files (e.g. `config.php.example`) are a good place to get default values for these fields if writing `pecan.xml` by hand.

Key R functions using these parameters are as follows:

- `PEcAn.DB::db.open` -- Open a database connection and create a connection object, which is used by many other functions for communicating with the PEcAn database.

#### `dbfiles`: Location of database files {#xml-dbfiles}

The `dbfiles` is a path to the local location of files needed to run models using PEcAn, including model executables and inputs.

```xml
  <database>
	...
    <dbfiles>/data/dbfiles</dbfiles>
	...
  </database>
```

#### (Experimental) `fia`: FIA database connection parameters {#xml-fia}

If a version of the FIA database is available, it can be configured using `<fia>` node, whose syntax is identical to that of the `<bety>` node.

```xml
  <database>
    ...
	<fia>
		<dbname>fia5data</dbname>
		<username>bety</username>
		<password>bety</password>
		<host>localhost</host>
	</fia>
	...
  </database>
```

Currently, this is only used for extraction of specific site vegetation information (notably, for ED2 `css`, `pss`, and `site` files).
Stability not ensured as of 1.5.3.

### `pft`: Plant functional type selection {#xml-pft}

The PEcAn system requires at least 1 plant functional type (PFT) to be specified inside the `<pfts>` section. 

```xml
  <pfts>
    <pft>
      <name>tundra.grasses</name> 
      <constants>
        <num>1</num>
      </constants>
      <posterior.files>Path to a post.distns.*.Rdata or prior.distns.Rdata</posterior.files>
    </pft>
  </pfts>
```

* `name` : (required) the name of the PFT, which must *exactly* match the name in the PEcAn database.
* `outdir`: (optional) Directory path in which PFT-specific output will be stored during meta-analysis and sensitivity analysis. If not specified (recommended), it will be written into `<outdir>/<pftname>`.
* `contants`: (optional) this section contains information that will be written directly into the model specific configuration files. For example, some models like ED2 use PFT numbers instead of names for PFTs, and those numbers can be specified here. See documentation for model-specific code for details.
* `posterior.files` (Optional) this tag helps to signal write.config functions to use specific posterior/prior files (such as HPDA or MA analysis) for generating samples without needing to access to the bety database.

``

This information is currently used by the following PEcAn workflow function:

- `get.traits` - ??????

### `meta.analysis`: Trait Meta Analysis {#xml-meta-analysis}

The section meta.analysis needs to exists for a meta.analysis to be executed, even though all tags inside are optional.
Conversely, if you do not want to do a trait meta-analysis (e.g. if you want to manually set all parameters), you should omit this node.

```xml
  <meta.analysis>
    <iter>3000</iter>
    <random.effects>
      <on>FALSE</on>
      <use_ghs>TRUE</use_ghs>
    </random.effects>
  </meta.analysis>

```

Some of the tags that can go in this section are:

* `iter`: [MCMC](http:/en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (Markov Chain Monte Carlo) chain length, i.e. the total number of posterior samples in the meta-analysis, default is 3000. Smaller numbers will run faster but produce larger errors.
* `random.effects`: Settings related to whether to include random effects (site, treatment) in meta-analysis model. 
- `on`: Default is set to FALSE to work around convergence problems caused by an over parameterized model (e.g. too many sites, not enough data). Can be turned to TRUE for including hierarchical random effects.
- `use_ghs`: Default is set to TRUE to include greenhouse measurements. Can be set to FALSE to exclude cases where all data is from greenhouse.
* `update`: Should previous results of meta.analysis and get.traits be re-used. If set to TRUE the meta-analysis and get.trait.data will always be executed. Setting this to FALSE will try and reuse existing results. Future versions will allow for AUTO as well which will try and reuse if the PFT/traits have not changed. The default value is FALSE.
* `threshold`: threshold for Gelman-Rubin convergence diagnostic (MGPRF); default is 1.2.

This information is currently used by the following PEcAn workflow function:

- `PEcAn.MA::run.meta.analysis` - ???

### `model`: Model configuration {#xml-model}

This section describes which model PEcAn should run and some instructions for how to run it.

```xml
<model>
	<id>7</id>
	<type>ED2</type>
	<binary>/usr/local/bin/ed2.r82</binary>
	<prerun>module load hdf5</prerun>
	<config.header>
		<!--...xml code passed directly to config file...-->
	</config.header>
</model>
```

Some important tags are as follows:

- `id` -- The unique numeric ID of the model in the PEcAn database `models` table. If this is present, then `type` and `binary` are optional since they can be determined from the PEcAn database.
- `type` -- The model "type", matching the PEcAn database `modeltypes` table `name` column. This also refers to which PEcAn model-specific package will be used. In PEcAn, a "model" refers to a specific version (e.g. release, git commit) of a specific model, and "model type" is used to link different releases of the same model. Model "types" also have specific PFT definitions and other requirements associated with them (e.g. the ED2 model "type" requires a global land cover database).
- `binary` -- The file path to the model executable. If omitted, PEcAn will use whatever path is registered in the PEcAn database for the current machine. 
- `prerun` -- Additional options added to the `job.sh` script, which is used to execute the model. This is useful for setting specific environment variables, load modules, etc.

This information is currently used by the following PEcAn workflow function:

- `PEcAn.<MODEL>::write.config.<MODEL>` -- Write [model-specific configuration files](#pecan-write-configs)
- `PEcAn.workflow::start_model_runs` -- Begin model run execution

#### Model-specific configuration {#xml-model-specific}

See the following:

* [ED2 Configuration](#models-ed)
* [SIPNET Configuration](#models-sipnet)
* [BIOCRO Configuration](#models-biocro)

#### ED2 specific tags {#xml-ed}

Following variables are ED specific and are used in the [ED2 Configuration](#models-ed).

Starting at 1.3.7 the tags for inputs have moved to `<run><inputs>`. This includes, veg, soil, psscss, inputs.

```xml
	<edin>/home/carya/runs/PEcAn_4/ED2IN.template</edin>
	<config.header>
		<radiation>
			<lai_min>0.01</lai_min>
		</radiation>
		<ed_misc>
			<output_month>12</output_month>
		</ed_misc> 
	</config.header>
	<phenol.scheme>0</phenol.scheme>
```

  
* **edin** : [required] template used to write ED2IN file
* **veg** : **OBSOLETE** [required] location of VEG database, now part of `<run><inputs>`
* **soil** : **OBSOLETE** [required] location of soild database, now part of `<run><inputs>`
* **psscss** : **OBSOLETE** [required] location of site inforation, now part of `<run><inputs>`. Should be specified as `<pss>`, `<css>` and `<site>`.
* **inputs** : **OBSOLETE** [required] location of additional input files (e.g. data assimilation data), now part of `<run><inputs>`. Should be specified as `<lu>` and `<thsums>`.

### `run`: Run Setup {#xml-run}

This section provides detailed configuration for the model run, including the site and time period for the simulation and what input files will be used.

```xml
  <run>
    <site>
      <id>1000000098</id>
      <met.start>2004/01/01</met.start>
      <met.end>2004/12/31</met.end>
      <site.pft>
        <pft.name>temperate.needleleaf.evergreen</pft.name>
        <pft.name>temperate.needleleaf.evergreen.test</pft.ame>
      </site.pft>
    </site>
    <inputs>
      <met>
        <source>CRUNCEP</source>
        <output>SIPNET</output>
      </met>
      <poolinitcond>
        <source>NEON_veg</source>
        <output>poolinitcond</output>
        <ensemble>100</ensemble>
        <startdate>2019-01-01</startdate>
        <enddate>2019-12-31</enddate>
        <storedir>/projectnb/dietzelab/neon_store</storedir>
      </poolinitcond>
    </inputs>
    <start.date>2004/01/01</start.date>
    <end.date>2004/12/31</end.date>
    <stop_on_error>TRUE</stop_on_error>
  </run>
```

#### `site`: Where to run the model {#xml-run-site}

This contains the following tags:

- `id` -- This is the numeric ID of the site in the PEcAn database (table `sites`, column `id`). PEcAn can automatically fill in other relevant information for the site (e.g. `name`, `lat`, `lon`) using the site ID, so those fields are optional if ID is provided.
- `name` -- The name of the site, as a string.
- `lat`, `lon` -- The latitude and longitude coordinates of the site, as decimals.
- `met.start`, `met.end` -- ???
- `<site.pft>` (optional) If this tag is found under the site tag, then PEcAn automatically makes sure that only PFTs defined under this tag is used for generating parameter's samples. Following shows an example of how this tag can be added to the PEcAn xml :

```xml
    <site.pft>
     <pft.name>temperate.needleleaf.evergreen</pft.name>
     <pft.name>temperate.needleleaf.evergreen</pft.name>
    </site.pft>
```
For multi-site runs if the `pft.site` tag (see {#xml-run-inputs}) is defined under `input`, then the above process will be done automatically under prepare settings step in PEcAn main workflow and there is no need for adding the tags manually. Using the `pft.site` tag however, requires a lookup table as an input (see {#xml-run-inputs}).


#### `inputs`: Model inputs {#xml-run-inputs}

Models require several different types of inputs to run.
Exact requirements differ from model to model, but common inputs include meteorological/climate drivers, site initial conditions (e.g. vegetation composition, carbon pools), and land use drivers.

In general, all inputs should have the following tags:

* `id`:  Numeric ID of the input in the PEcAn database (table `inputs`, column `id`). If not specified, PEcAn will try to figure this out based on the `source` tag (described below).
* `path`: The file path of the input. Usually, PEcAn will set this automatically based on the `id` (which, in turn, is determined from the `source`). However, this can be set manually for data that PEcAn does not know about (e.g. data that you have processed yourself and have not registered with the PEcAn database).
* `source`: The input data type. This tag name needs to match the names in the corresponding conversion functions. If you are using PEcAn's automatic input processing, this is the only field you need to set. However, this field is ignored if `id` and/or `path` are provided.
* `output`: ???

The following are the most common types of inputs, along with their corresponding tags:

##### `met`: Meteorological inputs {#xml-run-inputs-met}

(Under construction. See the `PEcAn.data.atmosphere` package, located in `modules/data.atmosphere`, for more details.)

##### (Experimental) `soil`: Soil inputs {#xml-run-inputs-soil}

(Under construction. See the `PEcAn.data.land` package, located in `modules/data.land`, for more details).

##### (Experimental) `veg`: Vegetation initial conditions {#xml-run-inputs-veg}

##### `poolinitcond`: initial condition inputs {#xml-run-inputs-poolinitcond}
* `source`: Data source of initial condition .rd veg files ex: NEON_veg, FIA.
* `output`: This tag can only must match the cooresponding tag in the modeltypes_formats table that matches with your model type ex for SIPNET model tag is poolinitcond.
* `ensemble`: Number of initial conditions ensemble member files ex: 30.

(Under construction. Follow developments in the `PEcAn.data.land` package, located in `modules/data.land` in the source code).

##### `pft.site` Multi-site site / PFT mapping

When performing multi-site runs, it is not uncommon to find that different sites need to be run with different PFTs, rather than running all PFTs at all sites. If you're interested to use a specific PFT for your site/sites you can use the following tag to tell PEcAn which PFT needs to be used for what site.


```
<pft.site>
  <path>site_pft.csv</path>
</pft.site>
```
For example using the above tag, user needs to have a csv file named `site_pft` stored in the pecan folder. At the moment we have functions supporting just the `.csv` and `.txt` files which are comma separated and have the following format:
```
site_id, pft_name
1000025731,temperate.broadleaf.deciduous
764,temperate.broadleaf.deciduous
```
Then pecan would use this lookup table to inform `write.ensemble.config` function about what PFTs need to be used for what sites.

#### `start.date` and `end.date`

The start and end date for the run, in a format parseable by R (e.g. `YYYY/MM/DD` or `YYYY-MM-DD`).
These dates are inclusive; in other words, they refer to the first and last days of the run, respectively.

NOTE: Any time-series inputs (e.g. meteorology drivers) must contain all of these dates.
PEcAn tries to detect and throw informative errors when dates are out of bounds inputs, but it may not know about some edge cases.

#### Other tags

The following tags are optional run settings that apply to any model: 

* `jobtemplate`: the template used when creating a `job.sh` file, which is used to launch the actual model. Each model has its own default template  in the `inst` folder of the corresponding R package (for instance, here is the one for [ED2](https://github.com/PecanProject/pecan/blob/main/models/ed/inst/template.job)). The following variables can be used: `@SITE_LAT@`, `@SITE_LON@`, `@SITE_MET@`, `@START_DATE@`, `@END_DATE@`, `@OUTDIR@`, `@RUNDIR@` which all come variables in the `pecan.xml` file. The following two command can be used to copy and clean the results from a scratch folder (specified as scratch in the run section below, for example local disk vs network disk) : `@SCRATCH_COPY@`, `@SCRATCH_CLEAR@`.

* `stop_on_error`: (logical) Whether the workflow should immediately terminate if _any_ of the model runs fail. If unset, this defaults to `TRUE` unless you are running an ensemble simulation (and ensemble size is greater than 1).

Some models also have model-specific tags, which are described in the [PEcAn Models](#pecan-models) section.

### `host`: Host information for remote execution {#xml-host}

This section provides settings for remote model execution, i.e. any execution that happens on a machine (including "virtual" machines, like Docker containers) different from the one on which the main PEcAn workflow is running.
A common use case for this section is to submit model runs as jobs to a high-performance computing cluster.
If no `host` tag is provided, PEcAn assumes models are run on `localhost`, a.k.a. the same machine as PEcAn itself.

For detailed instructions on remote execution, see the [Remote Execution](#pecan-remote) page.
For detailed information on configuring this for RabbitMQ, see the [RabbitMQ](#rabbitmq-xml) page.
The following provides a quick overview of XML tags related to remote execution.

**NOTE**: Any paths specified in the `pecan.xml` refer to paths on the `host` specified in this section, /not/ the machine on which PEcAn is running (unless models are running on `localhost` or this section is omitted).

```xml
	<host>
		<name>pecan2.bu.edu</name>
		<rundir>/fs/data3/guestuser/pecan/testworkflow/run</rundir>
		<outdir>/fs/data3/guestuser/pecan/testworkflow/out</outdir>
		<scratchdir>/tmp/carya</scratchdir>
		<clearscratch>TRUE</clearscratch>
		<qsub>qsub -N @NAME@ -o @STDOUT@ -e @STDERR@ -S /bin/bash</qsub>
		<qsub.jobid>Your job ([0-9]+) .*</qsub.jobid>
		<qstat>'qstat -j @JOBID@ &amp;> /dev/null || echo DONE'</qstat>
		<prerun>module load udunits R/R-3.0.0_gnu-4.4.6</prerun>
		<cdosetup>module load cdo/2.0.6</cdosetup>
		<modellauncher>
      <binary>/usr/local/bin/modellauncher</binary>
      <qsub.extra>-pe omp 20</qsub.extra>
		</modellauncher>
	</host>
```

The `host` section has the following tags:

* `name`: [optional] name of host server where model is located and executed, if not specified localhost is assumed.
* `folder`: [required] The location to store files on the remote machine. For localhost this is optional (`<outdir>` is the default), for any other host this is required.
* `rundir`: [optional] location where all the configuration files are written. If not specified, this will be generated starting at the path specified in `folder`.
* `outdir`: [optional] location where all the outputs of the model are written. If not specified, this will be generated starting at the path specified in `folder`.
* `scratchdir`: [optional] location where output is written. If specified the output from the model is written to this folder and copied to the outdir when the model is finished, this could significantly speed up the model execution (by using local or ram disk).
* `clearscratch`: [optional] if set to TRUE the scratchfolder is cleaned up after copying the results to the outdir, otherwise the folder will be left. The default is to clean up after copying.
* `qsub`: [optional] the command to submit a job to the queuing system. There are 3 parameters you can use when specifying the qsub command, you can add additional values for your specific setup (for example -l walltime to specify the walltime, etc). You can specify @NAME@ the pretty name, @STDOUT@ where to write stdout and @STDERR@, where to write stderr. You can specify an empty element (`<qsub/>`) in which case it will use the default value is `qsub -V -N @NAME@ -o @STDOUT@ -e @STDERR@ -s /bin/bash`.
* `qsub.jobid`: [optional] the regular expression used to find the `jobid` returned from `qsub`. If not specified (and `qsub` is) it will use the default value is `Your job ([0-9]+) .*`
* `qstat`: [optional] the command to execute to check if a job is finished, this should return DONE if the job is finished. There is one parameter this command should take `@JOBID@` which is the ID of the job as returned by `qsub.jobid`. If not specified (and qsub is) it will use the default value is `qstat -j @JOBID@ || echo DONE`
* `prerun`: [optional] additional options to add to the job.sh at the top.
* `cdosetup`: [optional] additional options to add to the job.sh at the top, specifically work for the CDO Linux command line package, allowing the user to generate the full year netCDF file through model2netcdf.SIPNET function (there has been an issue while this function will iteratively overlap the previous netCDF file, resulting in partial data loss).
* `modellauncher`: [optional] this is an experimental section that will allow you to submit all the runs as a single job to a HPC system.

The `modellauncher` section if specified will group all runs together and only submit a single job to the HPC cluster. This single job will leverage of a MPI program that will execute a single run. Some HPC systems will place a limit on the number of jobs that can be executed in parallel, this will only submit a single job (using multiple nodes). In case there is no limit on the number of jobs, a single PEcAn run could potentially submit a lot of jobs resulting in the full cluster running jobs for a single PEcAn run, preventing others from executing on the cluster.

The `modellauncher` has 3 arguments:
* `binary` : [required] The full path to the binary modellauncher. Source code for this file can be found in `pecan/contrib/modellauncher`](https://github.com/PecanProject/pecan/tree/develop/contrib/modellauncher).
* `qsub.extra` : [optional] Additional flags to pass to qsub besides those specified in the `qsub` tag in host. This option can be used to specify that the MPI environment needs to be used and the number of nodes that should be used.
* `mpirun`: [optional] Additional commands to be added to the front of `launcher.sh`.  Default is `mpirun <path to binary> <path to joblist.txt>`.  Edit this to, for example, load necessary modules to run `mpirun`.

## Advanced features {#xml-advanced}

### `ensemble`: Ensemble Runs {#xml-ensemble}

As with `meta.analysis`, if this section is missing, then PEcAn will not do an ensemble analysis.

```xml
  <ensemble>
    <size>1</size>
    <variable>NPP</variable>
    <samplingspace>
      <parameters>
        <method>uniform</method>
      </parameters>
      <met>
        <method>sampling</method>
      </met>
    </samplingspace>
  </ensemble>
```

An alternative configuration is as follows:

```xml
<ensemble>
  <size>5</size>
  <variable>GPP</variable>
  <start.year>1995</start.year>
  <end.year>1999</end.year>
  <samplingspace>
  <parameters>
    <method>lhc</method>
  </parameters>
  <met>
    <method>sampling</method>
  </met>
  </samplingspace>
</ensemble>
```

Tags in this block can be broken down into two categories: Those used for setup (which determine how the ensemble analysis runs) and those used for post-hoc analysis and visualization (i.e. which do not affect how the ensemble is generated).

Tags related to ensemble setup are:

* `size` : (required) the number of runs in the ensemble.
* `samplingspace`: (optional) Contains tags for defining how the ensembles will be generated.

Each piece in the sampling space can potentially have a method tag and a parent tag. Method refers to the sampling method and parent refers to the cases where we need to link the samples of two components. When no tag is defined for one component, one sample will be generated and used for all the ensembles. This allows for partitioning/studying different sources of uncertainties. For example, if no met tag is defined then, one met path will be used for all the ensembles and as a result the output uncertainty will come from the variability in the parameters. At the moment no sampling method is implemented for soil and vegetation.
Available sampling methods for `parameters` can be found in the documentation of the `PEcAn.utils::get.ensemble.samples` function.
For the cases where we need simulations with a predefined set of parameters, met and initial condition we can use the restart argument. Restart needs to be a list with name tags of `runid`, `inputs`, `new.params` (parameters), `new.state` (initial condition), `ensemble.id` (ensemble ids), `start.time`, and `stop.time`.

The restart functionality is developed using model specific functions by called `write_restart.modelname`. You need to make sure first that this function is already exist for your desired model.

Note: if the ensemble size is set to 1, PEcAn will select the **posterior median** parameter values rather than taking a single random draw from the posterior

Tags related to post-hoc analysis and visualization are:

* `variable`: (optional) name of one (or more) variables the analysis should be run for. If not specified, `sensitivity.analysis` variable is used, otherwise default is GPP (Gross Primary Productivity).

(NOTE: This static visualization functionality will soon be deprecated as PEcAn moves towards interactive visualization tools based on Shiny and htmlwidgets).

This information is currently used by the following PEcAn workflow functions:

- `PEcAn.<MODEL>::write.config.<MODEL>` - See [above](#pecan-write-configs).
- `PEcAn.uncertainty::write.ensemble.configs` - Write configuration files for ensemble analysis
- `PEcAn.uncertainty::run.ensemble.analysis` - Run ensemble analysis

### `sensitivity.analysis`: Sensitivity analysis {#xml-sensitivity-analysis}

Only if this section is defined a sensitivity analysis is done. This section will have `<quantile>` or `<sigma>` nodes. If neither are given, the default is to use the median +/- [1 2 3] x sigma (e.g. the 0.00135 0.0228 0.159 0.5 0.841 0.977 0.999 quantiles); If the 0.5 (median) quantile is omitted, it will be added in the code.

```xml
<sensitivity.analysis>
	<quantiles>
		<sigma>-3</sigma>
		<sigma>-2</sigma>
		<sigma>-1</sigma>
		<sigma>1</sigma>
		<sigma>2</sigma>
		<sigma>3</sigma>
	</quantiles>
  <variable>GPP</variable>
  <perpft>TRUE</perpft>
	<start.year>2004</start.year>
	<end.year>2006</end.year>
</sensitivity.analysis>
```

- `quantiles/sigma` : [optional] The number of standard deviations relative to the standard normal (i.e. "Z-score") for which to perform the ensemble analysis. For instance, `<sigma>1</sigma>` corresponds to the quantile associated with 1 standard deviation greater than the mean (i.e. 0.681). Use a separate `<sigma>` tag, all under the `<quantiles>` tag, to specify multiple quantiles. Note that we _do not automatically add the quantile associated with `-sigma`_ -- i.e. if you want +/- 1 standard deviation, then you must include both `<sigma>1</sigma>` _and_ `<sigma>-1</sigma>`.
- `start.date` : [required?] start date of the sensitivity analysis (in YYYY/MM/DD format) 
- `end.date` : [required?] end date of the sensitivity analysis (in YYYY/MM/DD format)
  - **_NOTE:_** `start.date` and `end.date` are distinct from values set in the run tag because this analysis can be done over a subset of the run.
-  `variable` : [optional] name of one (or more) variables the analysis should be run for. If not specified, sensitivity.analysis variable is used, otherwise default is GPP.
- `perpft` : [optional] if `TRUE` a sensitivity analysis on PFT-specific outputs will be run. This is only possible if your model provides PFT-specific outputs for the `variable` requested. This tag only affects the output processing, not the number of samples proposed for the analysis nor the model execution.

This information is currently used by the following PEcAn workflow functions:

- `PEcAn.<MODEL>::write.configs.<MODEL>` -- See [above](#pecan-write-configs)
- `PEcAn.uncertainty::run.sensitivity.analysis` -- Executes the uncertainty analysis

### Parameter Data Assimilation {#xml-parameter-data-assimilation}

The following tags can be used for parameter data assimilation. More detailed information can be found here: [Parameter Data Assimilation Documentation](#pda)

### Multi-Settings {#xml-multi-settings}

Multi-settings allows you to do multiple runs across different sites. This customization can also leverage site group distinctions to expedite the customization. It takes your settings and applies the same settings, changing only the site level tags across sites.

To start, add the multisettings tag within the `<run></run>` section of your xml
```
<multisettings>
  <multisettings>run</multisettings>
<multisettings> 
```
Additional tags for this section exist and can fully be seen here:
```
 <multisettings>
  <multisettings>assim.batch</multisettings>
  <multisettings>ensemble</multisettings>
  <multisettings>sensitivity.analysis</multisettings>
  <multisettings>run</multisettings>
 </multisettings>
 ```
These tags correspond to different pecan analysis that need to know that there will be multiple settings read in.
 
 
Next you'll want to add the following tags to denote the group of sites you want to use. It leverages site groups, which are defined in BETY.

```xml
 <sitegroup>
   <id>1000000022</id>
 </sitegroup>
````
If you add this tag, you must remove the `<site> </site>` tags from the `<run>` tag portion of your xml. 
The id of your sitegroup can be found by lookig up your site group within BETY.

You do not have to use the sitegroup tag. You can manually add multiple sites using the structure in the example below. 

Lastly change the top level tag to `<pecan.multi>`, meaning the top and bootom of your xml should look like this:

```
<?xml version="1.0"?>
<pecan.multi>
...
</pecan.multi>
```

Once you have defined these tags, you can run PEcAn, but there may be further specifications needed if you know that different data sources have different dates available.

Run workflow.R up until
```
# Write pecan.CHECKED.xml
PEcAn.settings::write.settings(settings, outputfile = "pecan.CHECKED.xml")
```
Once this section is run, you'll need to open `pecan.CHECKED.xml`. You will notice that it has expanded from your original `pecan.xml`.

```xml
 <run>
  <settings.1>
   <site>
    <id>796</id>
    <met.start>2005/01/01</met.start>
    <met.end>2011/12/31</met.end>
    <name>Bartlett Experimental Forest (US-Bar)</name>
    <lat>44.06464</lat>
    <lon>-71.288077</lon>
   </site>
   <start.date>2005/01/01</start.date>
   <end.date>2011/12/31</end.date>
   <inputs>
    <met>
     <path>/fs/data1/pecan.data/dbfiles/AmerifluxLBL_SIPNET_site_0-796/AMF_US-Bar_BASE_HH_4-1.2005-01-01.2011-12-31.clim</path>
    </met>
   </inputs>
  </settings.1>
  <settings.2>
   <site>
    <id>767</id>
    <met.start>2001/01/01</met.start>
    <met.end>2014/12/31</met.end>
    <name>Morgan Monroe State Forest (US-MMS)</name>
    <lat>39.3231</lat>
    <lon>-86.4131</lon>
   </site>
   <start.date>2001/01/01</start.date>
   <end.date>2014/12/31</end.date>
   <inputs>
    <met>
     <path>/fs/data1/pecan.data/dbfiles/AmerifluxLBL_SIPNET_site_0-767/AMF_US-MMS_BASE_HR_8-1.2001-01-01.2014-12-31.clim</path>
    </met>
   </inputs>
  </settings.2>
....
</run>
```
* The `...` replaces the rest of the site settings for however many sites are within the site group.

Looking at the example above, take a close look at the `<met.start></met.start>` and `<met.end></met.end>`. You will notice that for both sites, the dates are different. In this example they were edited by hand to include the dates that are available for that site and source. You must know your source prior. Only the source CRUNCEP has a check that will tell you if your dates are outside the range available. PEcAn will automatically populate these dates across sites according the original setting of start and end dates.


In addition, you will notice that the `<path></path>` section contains the model specific meteorological data file. You can add that in by hand or you can you can leave the normal tags that met process workflow will use to process the data into your model specific format:
```
<met>
  <source>AmerifluxLBL</source>
  <output>SIPNET</output>
  <username>pecan</username>
</met>
```


### (experimental) State Data Assimilation {#xml-state-data-assimilation}

The following tags can be used for state data assimilation. More detailed information can be found here: [State Data Assimilation Documentation](#sda)

```xml
<state.data.assimilation>
	<process.variance>TRUE</process.variance>
	<aqq.Init>1</aqq.Init>
	<bqq.Init>1</bqq.Init>
  <sample.parameters>FALSE</sample.parameters>
  <adjustment>TRUE</adjustment>
  <censored.data>FALSE</censored.data>
  <FullYearNC>TRUE</FullYearNC>
  <NC.Overwrite>FALSE</NC.Overwrite>
  <NC.Prefix>sipnet.out</NC.Prefix>
  <q.type>SINGLE</q.type>
  <free.run>FALSE</free.run>
  <Localization.FUN>Local.support</Localization.FUN>
  <scalef>1</scalef>
  <chains>5</chains>
  <state.variables>
   <variable>
    <variable.name>AbvGrndWood</variable.name>
    <unit>MgC/ha</unit>
    <min_value>0</min_value>
    <max_value>9999</max_value>
   </variable>
   <variable>
    <variable.name>LAI</variable.name>
    <unit></unit>
    <min_value>0</min_value>
    <max_value>9999</max_value>
   </variable>
   <variable>
  <variable.name>SoilMoistFrac</variable.name>
  <unit></unit>
  <min_value>0</min_value>
  <max_value>100</max_value>
  </variable>
   <variable>
    <variable.name>TotSoilCarb</variable.name>
    <unit>kg/m^2</unit>
    <min_value>0</min_value>
    <max_value>9999</max_value>
   </variable>
  </state.variables>
  <Obs_Prep>
   <Landtrendr_AGB>
    <AGB_input_dir>/projectnb/dietzelab/dongchen/Multi-site/download_500_sites/AGB</AGB_input_dir>
    <allow_download>TRUE</allow_download>
    <export_csv>TRUE</export_csv>
    <timestep>
     <unit>year</unit>
     <num>1</num>
    </timestep>
   </Landtrendr_AGB>
   <MODIS_LAI>
    <search_window>30</search_window>
    <export_csv>TRUE</export_csv>
    <run_parallel>TRUE</run_parallel>
    <timestep>
     <unit>year</unit>
     <num>1</num>
    </timestep>
   </MODIS_LAI>
   <SMAP_SMP>
    <search_window>30</search_window>
    <export_csv>TRUE</export_csv>
    <update_csv>FALSE</update_csv>
    <timestep>
     <unit>year</unit>
     <num>1</num>
    </timestep>
   </SMAP_SMP>
   <Soilgrids_SoilC>
    <timestep>
     <unit>year</unit>
     <num>1</num>
    </timestep>
   </Soilgrids_SoilC>
   <outdir>/projectnb/dietzelab/dongchen/All_NEON_SDA/test_OBS</outdir>
   <start.date>2012-07-15</start.date>
   <end.date>2021-07-15</end.date>
  </Obs_Prep>
  <spin.up>
  	<start.date>2004/01/01</start.date>
	  <end.date>2006/12/31</end.date>
  </spin.up>
  <forecast.time.step>1</forecast.time.step>
	<start.date>2004/01/01</start.date>
	<end.date>2006/12/31</end.date>
</state.data.assimilation>
```

* **process.variance** : [optional] TRUE/FLASE flag for if process variance should be estimated (TRUE) or not (FALSE). If TRUE, a generalized ensemble filter will be used. If FALSE, an ensemble Kalman filter will be used. Default is FALSE.
* **aqq.Init** : [optional] The initial value of `aqq` used for estimate the Q distribution, the default value is 1 (note that, the `aqq.init` and `bqq.init` right now only work on the `VECTOR` q type, and we didn't account for the variabilities of them across sites or variables, meaning we initialize the `aqq` and `bqq` given single value).
* **bqq.Init** : [optional] The initial value of `bqq` used for estimate the Q distribution, the default value is 1.
* **sample.parameters** : [optional] TRUE/FLASE flag for if parameters should be sampled for each ensemble member or not. This allows for more spread in the intial conditions of the forecast.
* **adjustment** : [optional] Bool variable decide if you want to adjust analysis results by the likelihood.
* **censored.data** : [optional] Bool variable decide if you want to do MCMC sampling for the forecast ensemble space, the default is FALSE.
* **FullYearNC** : [optional] Bool variable decide if you want to generate the full-year netcdf file when there is a overlap in time, the default is TRUE.
* **NC.Overwrite** : [optional] Bool variable decide if you want to overwrite the previous netcdf file when there is a overlap in time, the default is FALSE.
* **NC.Prefix** : [optional] The prefix for the generation of the full-year netcdf file, the default is sipnet.out.
* **q.type** : [optional] The type of process variance that will be estimated, the default is SINGLE.
* **free.run** : [optional] If it's a free run without any observations, the default is FALSE.
* **Localization.FUN** : [optional] The localization function name for the localization operation, the default is Local.support.
* **scalef** : [optional] The scale parameter used for the localization operation, the smaller the value is, the sites are more isolated.
* **chains** : [optional] The number of chains needed to be estimated during the MCMC sampling process.
* **_NOTE:_** If TRUE, you must also assign a vector of trait names to pick.trait.params within the sda.enkf function.
* **state.variable** : [required] State variable that is to be assimilated (in PEcAn standard format, with pre-specified variable name, unit, and range). Four variables can be assimilated so far: including Aboveground biomass (AbvGrndWood), LAI, SoilMoistFrac, and Soil carbon (TotSoilCarb).
* **Obs_Prep** : [required] This section will be handled through the SDA_Obs_Assembler function, if you want to proceed with this function, this section is required.
* **spin.up** : [required] start.date and end.date for model spin up.
* **_NOTE:_** start.date and end.date are distinct from values set in the run tag because spin up can be done over a subset of the run.
* **forecast.time.step** : [optional] start.date and end.date for model spin up.
* **start.date** : [required?] start date of the state data assimilation (in YYYY/MM/DD format) 
* **end.date** : [required?] end date of the state data assimilation (in YYYY/MM/DD format)
* **_NOTE:_** start.date and end.date are distinct from values set in the run tag because this analysis can be done over a subset of the run.

### State Variables for State Data Assimilation {#xml-state-variables}

The following tags can be used for documentation of state variables required for the SDA workflow.

```xml
<state.data.assimilation>
  <state.variables>
   <variable>
    <variable.name>AbvGrndWood</variable.name>
    <unit>MgC/ha</unit>
    <min_value>0</min_value>
    <max_value>9999</max_value>
   </variable>
   <variable>
    <variable.name>LAI</variable.name>
    <unit></unit>
    <min_value>0</min_value>
    <max_value>9999</max_value>
   </variable>
   <variable>
  <variable.name>SoilMoistFrac</variable.name>
  <unit></unit>
  <min_value>0</min_value>
  <max_value>100</max_value>
  </variable>
   <variable>
    <variable.name>TotSoilCarb</variable.name>
    <unit>kg/m^2</unit>
    <min_value>0</min_value>
    <max_value>9999</max_value>
   </variable>
  </state.variables>
</state.data.assimilation>
```

* **variable** : [required] Each of this section should include each state variables separately.
* **variable.name** : [required] The name of the state variable (same as what we call it in the model output file. for example, "SIPNET.out" file).
* **unit** : [required] The unit of the state variable (can be empty if it's unitless).
* **min_value** : [required] The minimum range of the state variable.
* **max_value** : [required] The maximum range of the state variable.

### Observation Preparation for State Data Assimilation {#xml-observation-preparation}

The following tags can be used for observation preparation for the SDA workflow.

```xml
<state.data.assimilation>
  <Obs_Prep>
   <Landtrendr_AGB>
    <AGB_indir>/projectnb/dietzelab/dongchen/Multi-site/download_500_sites/AGB</AGB_indir>
    <allow_download>TRUE</allow_download>
    <export_csv>TRUE</export_csv>
    <timestep>
     <unit>year</unit>
     <num>1</num>
    </timestep>
   </Landtrendr_AGB>
   
   <MODIS_LAI>
    <search_window>30</search_window>
    <export_csv>TRUE</export_csv>
    <run_parallel>TRUE</run_parallel>
    <timestep>
     <unit>year</unit>
     <num>1</num>
    </timestep>
   </MODIS_LAI>
   
   <SMAP_SMP>
    <search_window>30</search_window>
    <export_csv>TRUE</export_csv>
    <update_csv>FALSE</update_csv>
    <timestep>
     <unit>year</unit>
     <num>1</num>
    </timestep>
   </SMAP_SMP>
   
   <Soilgrids_SoilC>
    <timestep>
     <unit>year</unit>
     <num>1</num>
    </timestep>
   </Soilgrids_SoilC>
   <outdir>/projectnb/dietzelab/dongchen/All_NEON_SDA/test_OBS</outdir>
   <start.date>2012-07-15</start.date>
   <end.date>2021-07-15</end.date>
  </Obs_Prep>
</state.data.assimilation>
```

The `Obs_Prep` section is specifically designed for the SDA_OBS_Assembler function within the `state.data.assimilation` section, which helps construct the obs.mean and obs.cov objects more efficiently. Within the `Obs_Prep` section, the user needs to specify data streams that will be proceeded in the assembler function as `DataSource_AbbreviationOfData` (for example: `Landtrendr_AGB` for Landtrendr aboveground biomass observations, or `Soilgrids_SoilC` for soilgrids soil organic carbon concentration). There are four functions that can be used for preparing observations: `MODIS_LAI_prep`, `Soilgrids_SoilC_prep`, `SMAP_SMP_prep`, and `Landtrendr_AGB_prep` functions.
Other than that, the output directory, and start and end date need to be set within the `Obs_Prep` section, note that if you choose to export csv file for any data, you will get the corresponding csv file just under the outdir, for the Rdata of obs.mean and obs.cov objects, they will be stored whthin the Rdata folder within the outdir, if the Rdata folder is not created, then the assembler will help to create this folder automatically. 
* **outdir** : [required] This tag is for every observation preparation function, which points to the directory where the csv file associated with each data stream should be generated, read, or updated.
* **start.date** : [required] This tag defines the exact start point of time, which differs from the start.date in the state.data.assimilation section in a way that it is the time when first observation should be taken.
* **end.date** : [required] This tag defines the exact end point of time, which differs from the end.date in the state.data.assimilation section in a way that it is the time when last observation should be taken.

Within each data-specific section (for example, within `Landtrendr_AGB` section), there are few general settings need to be set like the `export_csv` and `timestep` arguments.
* **export_csv** : [optional] This tag is for every observation preparation function, which decide if we want to export the csv file to the outdir. Normally the outdir should be given in order to set export_csv as TRUE. The default value is TRUE.
* **timestep** : [optional] This tag defines the time unit and how many unit time per each time step. It supports year, month, week, and day as time unit. If you assign a general timestep just under the Obs_Prep section, all variable will be handled with the same timestep. If you assign each variable with each timestep, then the assembler function will create obs.mean and obs.cov with mixed temporal resolution. The default value is list(unit="year", num=1).

There are also several settings that are data-specific, for example, the `AGB_indir` only applies to Landtrendr AGB data that points to the directory where the AGB files are stored. Beyond that, the allow_download is also AGB specific, which decide if you want to download the AGB files if some are missing.
* **AGB_indir** : [required] This tag is only for Landtrendr_AGB_prep function, which points to the directory where the pre-downloaded AGB files existed.
* **allow_download** : [optional] This tag is only for Landtrendr_AGB_prep function, which decide if we want to proceed to download the AGB files (that will take a while, and a lot of space) if some of them are detected to be empty. The default value is FALSE.

For the MODIS LAI and SMAP soil moisture observations, the `search_window` specifies how many days you want to search for the data with desired dates. Beyond that, the `update_csv` is specifically used for the SMAP data, which decide if you want to reproduce formatted csv file based on the downloaded `SMAP_gee.csv` file. For the `run_parallel` in MODIS LAI settings, it specify if you want to proceed the function parallely.
* **search_window** : [optional] This tag is used for LAI and SMAP observation preparation functions, which defines the search window within which the closest observation to the target date is selected. The default value is 30 days.
* **update_csv** : [optional] This tag is used for SMAP preparation function only, which decide if we want to update the CSV file. Normally, we will only set it to be TRUE if we redownload the SMAP_GEE csv file from Google Earth Engine, the tutorial can be found in the SMAP_SMP_prep function itself. The default value is FALSE.
* **run_parallel** : [optional] This tag defines if you want to proceed the MODIS LAI function parallely, the default value is FALSE.


### (experimental) Benchmarking {#xml-benchmarking}

Coming soon...

### Remote data module {#xml-remote_process}

This section describes the tags required for configuring `remote_process`.

```xml
  <remotedata>
  <out_get_data>...</out_get_data>
  <source>...</source>
  <collection>...</collection>
  <scale>...</scale>
  <projection>...</projection>
  <qc>...</qc>
  <algorithm>...</algorithm>
  <credfile>...</credfile>
  <out_process_data>...</out_process_data>
  <overwrite>...</overwrite>
  </remotedata>
```

* `out_get_data`: (required) type of raw output requested, e.g, bands, smap
* `source`: (required) source of remote data, e.g., gee or AppEEARS 
* `collection`: (required) dataset or product name as it is provided on the source, e.g. "COPERNICUS/S2_SR" for gee or "SPL3SMP_E.003" for AppEEARS
* `scale`: (optional) pixel resolution required for some gee collections, recommended to use 10 for Sentinel 2
* `projection`: (optional) type of projection. Only required for AppEEARS polygon AOI type
* `qc`: (optional) quality control parameter, required for some gee collections
* `overwrite`: (optional) if TRUE database checks will be skipped and existing data of same type will be replaced entirely. When processed data is requested, the raw data required   for creating it will also be replaced. By default FALSE

These tags are only required if processed data is requested:

* `out_process_data`: (optional) type of processed output requested, e.g, LAI
* `algorithm`: (optional) algorithm used for processing data, currently only SNAP is implemented to estimate LAI from Sentinel-2 bands
* `credfile`: (optional) absolute path to JSON file containing Earthdata username and password, only required for AppEEARS
* `pro_mimetype`: (optional) MIME type of the processed file
* `pro_formatname`: (optional) format name of the processed file

Additional information for the module are taken from the registration files located at `data.remote/inst/registration`

The output data from the module are returned in the following tags:

* `raw_id`: input id of the raw file
* `raw_path`: absolute path to the raw file
* `pro_id`: input id of the processed file
* `pro_path`: absolute path to the processed file
