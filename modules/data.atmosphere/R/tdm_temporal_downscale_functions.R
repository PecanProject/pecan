##' @family tdm - Temporally Downscale Meteorology
##' @param dat.train - training data generated by tdm_nc2dat.train.R
##' @param n.beta - number of betas to generate
##' @param path.out - path to where the training models & betas will be stored
##' @param resids - whether or not to propogate residuals, set to FALSE
##' @param parallel - whether or not to run in parallel. this is a feature
##'                   still being worked on, set to FALSE
##' @param n.cores - number of cores to use parallel processing on, set to NULL
##' @param day.window - number of days surrounding current day we want to pull
##'                     statistics from
##' @param seed - allows this to be reproducible

##' @author Christy Rollinson, James Simkins

#----------------------------------------------------------------------
# Begin Script
#----------------------------------------------------------------------


model.air_temperature <- function(dat.train, n.beta, path.out, day.window, 
    resids = F, parallel = F, n.cores = NULL, seed = 1237) {
    
    set.seed(seed)
    if (!dir.exists(path.out)) 
        dir.create(path.out, recursive = T)
    
    # The model we're going to use
    model.train <- function(dat.subset, n.beta, resids = resids) {
        dat.subset$year <- as.ordered(dat.subset$year)

        mod.doy <- lm(air_temperature ~ as.ordered(hour) * air_temperature_max.day * 
            (lag.air_temperature + lag.air_temperature_min + air_temperature_min.day) + 
            as.ordered(hour) * air_temperature_min.day * next.air_temperature_max - 
            1 - as.ordered(hour) - lag.air_temperature - lag.air_temperature_min - 
            next.air_temperature_max - air_temperature_max.day - air_temperature_min.day, 
            data = dat.subset)  #
        
        # If we can't estimate the covariance matrix, double our data and try
        # again NOTE: THIS IS NOT A GOOD PERMANENT FIX!!
        if (is.na(summary(mod.doy)$adj.r.squared)) {
            warning(paste0("Can not estimate covariance matrix for day of year: ", 
                unique(dat.subset$doy)))
            dat.subset <- rbind(dat.subset, dat.subset)
            mod.doy <- lm(air_temperature ~ as.ordered(hour) * air_temperature_max.day * 
                (lag.air_temperature + lag.air_temperature_min + air_temperature_min.day) + 
                as.ordered(hour) * air_temperature_min.day * next.air_temperature_max - 
                1 - as.ordered(hour) - lag.air_temperature - lag.air_temperature_min - 
                next.air_temperature_max - air_temperature_max.day - air_temperature_min.day, 
                data = dat.subset)  #
        }
        
        # Generate a bunch of random coefficients that we can pull from without
        # needing to do this step every day
        mod.coef <- coef(mod.doy)
        mod.cov <- vcov(mod.doy)
        piv <- as.numeric(which(!is.na(mod.coef)))
        Rbeta <- MASS::mvrnorm(n = n.beta, mod.coef[piv], mod.cov)
        
        list.out <- list(model = mod.doy, betas = Rbeta)
        
        
        # Model residuals as a function of hour so we can increase our
        # uncertainty
        if (resids == T) {
            dat.subset[!is.na(dat.subset$lag.air_temperature) & !is.na(dat.subset$next.air_temperature_max), 
                "resid"] <- resid(mod.doy)
            resid.model <- lm(resid ~ as.factor(hour) * (air_temperature_max.day * 
                air_temperature_min.day) - 1, data = dat.subset[!is.na(dat.subset$lag.air_temperature), 
                ])
            res.coef <- coef(resid.model)
            res.cov <- vcov(resid.model)
            res.piv <- as.numeric(which(!is.na(res.coef)))
            
            beta.resid <- MASS::mvrnorm(n = n.beta, res.coef[res.piv], 
                res.cov)
            
            list.out[["model.resid"]] <- resid.model
            list.out[["betas.resid"]] <- beta.resid
        }
        return(list.out)
        
    }
    
    dat.list <- list()
    mod.out <- list()
    
    
    # Make the data into a list Training the model on ax X-day window
    # around the actual DOY we're trying to model this helps avoid problems
    # with lack of data in small datasets like Ameriflux Default window is
    # 5 days (+/- 2)
    
    
    for (i in unique(dat.train$doy)) {
        if (i >= 365) {
            # Lump leap day in with non-leap Dec 31
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= 365 - day.window/2 | 
                dat.train$doy <= day.window/2, ]
        } else if (i == 1) {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy <= i + day.window/2 | 
                dat.train$doy >= 365 - day.window/2, ]
        } else {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= i - day.window/2 & 
                dat.train$doy <= i + day.window/2, ]
        }
    }
    
    # Do the computation and save a list Final list will have 2 layers per
    # DOY: the model, and a bunch of simulated betas
    if (parallel == T) {
        warning("Running model calculation in parallel.  This WILL crash if you do not have access to a LOT of memory!")
        library(parallel)
        mod.out <- parallel::mclapply(dat.list, model.train, mc.cores = n.cores, 
            n.beta = n.beta, resids = resids)
        
        for (i in names(mod.out)) {
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_air_temperature_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[[i]][["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[[i]][["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[[i]][["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out[[i]][["model"]]
            save(mod.save, file = file.path(path.out, paste0("model_air_temperature_", 
                i, ".Rdata")))
        }
    } else {
        for (i in names(dat.list)) {
            mod.out <- model.train(dat.subset = dat.list[[i]], n.beta = n.beta, 
                resids = resids)
            
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_air_temperature_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out$mode
            save(mod.save, file = file.path(path.out, paste0("model_air_temperature_", 
                i, ".Rdata")))
        }
    }
    # return(mod.out)
}

model.surface_downwelling_shortwave_flux_in_air <- function(dat.train, 
    n.beta, path.out, resids = F, parallel = F, n.cores = NULL, day.window = 5, 
    seed = 1341) {
    
    set.seed(seed)
    if (!dir.exists(path.out)) 
        dir.create(path.out, recursive = T)
    # The model we're going to use
    model.train <- function(dat.subset, threshold = NULL, n.beta, resids = resids) {
        
        # Don't bother trying to fit hours that are completely or pretty darn
        # close to dark
        hrs.day <- unique(dat.subset[dat.subset$surface_downwelling_shortwave_flux_in_air > 
            threshold, "hour"])
        
        # Note: played around with a log-transformation of
        # surface_downwelling_shortwave_flux_in_air to prevent negative values,
        # but that resulted in bias at upper range Solution was to just say
        # anything <0 = 0 mod.doy <-
        # lm(surface_downwelling_shortwave_flux_in_air ~
        # as.factor(hour)*surface_downwelling_shortwave_flux_in_air.day,
        # data=dat.subset[dat.subset$hour %in% hrs.day,]) ###
        mod.doy <- lm(surface_downwelling_shortwave_flux_in_air ~ as.factor(hour) * 
            surface_downwelling_shortwave_flux_in_air.day - 1 - surface_downwelling_shortwave_flux_in_air.day - 
            as.factor(hour), data = dat.subset[dat.subset$hour %in% hrs.day, 
            ])  ###
        
        # If we can't estimate the covariance matrix, double our data and try
        # again NOTE: THIS IS NOT A GOOD PERMANENT FIX!!
        if (is.na(summary(mod.doy)$adj.r.squared)) {
            warning(paste0("Can not estimate covariance matrix for day of year: ", 
                unique(dat.subset$doy)))
            dat.subset <- rbind(dat.subset, dat.subset)
            mod.doy <- lm(surface_downwelling_shortwave_flux_in_air ~ as.factor(hour) * 
                surface_downwelling_shortwave_flux_in_air.day - 1 - surface_downwelling_shortwave_flux_in_air.day - 
                as.factor(hour), data = dat.subset[dat.subset$hour %in% 
                hrs.day, ])  ###
        }
        
        # Generate a bunch of random coefficients that we can pull from without
        # needing to do this step every day
        mod.coef <- coef(mod.doy)
        mod.cov <- vcov(mod.doy)
        piv <- as.numeric(which(!is.na(mod.coef)))
        Rbeta <- MASS::mvrnorm(n = n.beta, mod.coef[piv], mod.cov)
        
        list.out <- list(model = mod.doy, betas = Rbeta)
        # Model residuals as a function of hour so we can increase our
        # uncertainty
        if (resids == T) {
            dat.subset[dat.subset$hour %in% hrs.day, "resid"] <- resid(mod.doy)
            resid.model <- lm(resid ~ as.factor(hour) * surface_downwelling_shortwave_flux_in_air.day - 
                1, data = dat.subset[dat.subset$hour %in% hrs.day, ])
            res.coef <- coef(resid.model)
            res.cov <- vcov(resid.model)
            res.piv <- as.numeric(which(!is.na(res.coef)))
            
            beta.resid <- MASS::mvrnorm(n = n.beta, res.coef[res.piv], 
                res.cov)
            
            list.out[["model.resid"]] <- resid.model
            list.out[["betas.resid"]] <- beta.resid
            
        }
        
        return(list.out)
    }
    
    dat.list <- list()
    mod.out <- list()
    
    # Make the data into a list Training the model on ax X-day window
    # around the actual DOY we're trying to model this helps avoid problems
    # with lack of data in small datasets like Ameriflux Default window is
    # 5 days (+/- 2)
    for (i in unique(dat.train$doy)) {
        if (i >= 365) {
            # Lump leap day in with non-leap Dec 31
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= 365 - day.window/2 | 
                dat.train$doy <= day.window/2, ]
        } else if (i == 1) {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy <= i + day.window/2 | 
                dat.train$doy >= 365 - day.window/2, ]
        } else {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= i - day.window/2 & 
                dat.train$doy <= i + day.window/2, ]
        }
    }
    
    # Do the computation and save a list Final list will have 2 layers per
    # DOY: the model, and a bunch of simulated betas
    if (parallel == T) {
        warning("Running model calculation in parallel.  This WILL crash if you do not have access to a LOT of memory!")
        library(parallel)
        mod.out <- parallel::mclapply(dat.list, model.train, mc.cores = n.cores, 
            n.beta = n.beta, resids = resids, threshold = quantile(dat.train[dat.train$surface_downwelling_shortwave_flux_in_air > 
                0, "surface_downwelling_shortwave_flux_in_air"], 0.05))
        
        # Use a loop to sace each day of year independently
        for (i in names(mod.out)) {
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_surface_downwelling_shortwave_flux_in_air_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[[i]][["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[[i]][["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[[i]][["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out[[i]][["model"]]
            save(mod.save, file = file.path(path.out, paste0("model_surface_downwelling_shortwave_flux_in_air_", 
                i, ".Rdata")))
        }
        
    } else {
        for (i in names(dat.list)) {
            mod.out <- model.train(dat.subset = dat.list[[i]], threshold = quantile(dat.train[dat.train$surface_downwelling_shortwave_flux_in_air > 
                0, "surface_downwelling_shortwave_flux_in_air"], 0.05), 
                n.beta, resids = resids)
            
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_surface_downwelling_shortwave_flux_in_air_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out$mode
            save(mod.save, file = file.path(path.out, paste0("model_surface_downwelling_shortwave_flux_in_air_", 
                i, ".Rdata")))
        }
    }
    
    return(mod.out)
}

model.surface_downwelling_longwave_flux_in_air <- function(dat.train, n.beta, 
    path.out, resids = F, parallel = F, n.cores = NULL, day.window = 5, 
    seed = 341) {
    
    set.seed(seed)
    if (!dir.exists(path.out)) 
        dir.create(path.out, recursive = T)
    
    # The model we're going to use
    model.train <- function(dat.subset, n.beta, resids = resids) {
        
        # mod.doy <- lm(surface_downwelling_longwave_flux_in_air ~
        # as.factor(hour)*surface_downwelling_longwave_flux_in_air.day*(lag.surface_downwelling_longwave_flux_in_air
        # + next.surface_downwelling_longwave_flux_in_air +
        # surface_downwelling_shortwave_flux_in_air.day +
        # air_temperature_max.day + air_temperature_min.day) - as.factor(hour)
        # - air_temperature_max.day - air_temperature_min.day -
        # surface_downwelling_shortwave_flux_in_air.day - 1, data=dat.subset)
        # ###
        mod.doy <- lm(sqrt(surface_downwelling_longwave_flux_in_air) ~ 
            as.factor(hour) * surface_downwelling_longwave_flux_in_air.day * 
                (lag.surface_downwelling_longwave_flux_in_air + next.surface_downwelling_longwave_flux_in_air) - 
                as.factor(hour) - 1 - lag.surface_downwelling_longwave_flux_in_air - 
                next.surface_downwelling_longwave_flux_in_air - surface_downwelling_longwave_flux_in_air.day - 
                surface_downwelling_longwave_flux_in_air.day * lag.surface_downwelling_longwave_flux_in_air - 
                surface_downwelling_longwave_flux_in_air.day * next.surface_downwelling_longwave_flux_in_air, 
            data = dat.subset)  ###
        
        # If we can't estimate the covariance matrix, stop & increase the
        # moving window
        if (is.na(summary(mod.doy)$adj.r.squared)) {
            stop(paste0("Can not estimate covariance matrix for day of year: ", 
                unique(dat.subset$doy), ";  Increase day.window and try again"))
        }
        
        # Generate a bunch of random coefficients that we can pull from without
        # needing to do this step every day
        mod.coef <- coef(mod.doy)
        mod.cov <- vcov(mod.doy)
        piv <- as.numeric(which(!is.na(mod.coef)))
        Rbeta <- MASS::mvrnorm(n = n.beta, mod.coef[piv], mod.cov)
        
        list.out <- list(model = mod.doy, betas = Rbeta)
        # Model residuals as a function of hour so we can increase our
        # uncertainty
        if (resids == T) {
            dat.subset[!is.na(dat.subset$lag.surface_downwelling_longwave_flux_in_air) & 
                !is.na(dat.subset$next.surface_downwelling_longwave_flux_in_air), 
                "resid"] <- resid(mod.doy)
            resid.model <- lm(resid ~ as.factor(hour) * surface_downwelling_longwave_flux_in_air.day - 
                1, data = dat.subset[, ])
            res.coef <- coef(resid.model)
            res.cov <- vcov(resid.model)
            res.piv <- as.numeric(which(!is.na(res.coef)))
            
            beta.resid <- MASS::mvrnorm(n = n.beta, res.coef[res.piv], 
                res.cov)
            
            list.out[["model.resid"]] <- resid.model
            list.out[["betas.resid"]] <- beta.resid
        }
        
        return(list.out)
    }
    
    dat.list <- list()
    mod.out <- list()
    
    # Make the data into a list Training the model on ax X-day window
    # around the actual DOY we're trying to model this helps avoid problems
    # with lack of data in small datasets like Ameriflux Default window is
    # 5 days (+/- 2)
    for (i in unique(dat.train$doy)) {
        if (i >= 365) {
            # Lump leap day in with non-leap Dec 31
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= 365 - day.window/2 | 
                dat.train$doy <= day.window/2, ]
        } else if (i == 1) {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy <= i + day.window/2 | 
                dat.train$doy >= 365 - day.window/2, ]
        } else {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= i - day.window/2 & 
                dat.train$doy <= i + day.window/2, ]
        }
    }
    
    # Do the computation and save a list Final list will have 2 layers per
    # DOY: the model, and a bunch of simulated betas
    if (parallel == T) {
        library(parallel)
        warning("Running model calculation in parallel.  This WILL crash if you do not have access to a LOT of memory!")
        mod.out <- parallel::mclapply(dat.list, model.train, mc.cores = n.cores, 
            n.beta = n.beta, resids = resids)
        
        # Use a loop to sace each day of year independently
        for (i in names(mod.out)) {
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_surface_downwelling_longwave_flux_in_air_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[[i]][["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[[i]][["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[[i]][["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out[[i]][["model"]]
            save(mod.save, file = file.path(path.out, paste0("model_surface_downwelling_longwave_flux_in_air_", 
                i, ".Rdata")))
        }
    } else {
        for (i in names(dat.list)) {
            mod.out <- model.train(dat.subset = dat.list[[i]], n.beta = n.beta, 
                resids = resids)
            
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_surface_downwelling_longwave_flux_in_air_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out$mode
            save(mod.save, file = file.path(path.out, paste0("model_surface_downwelling_longwave_flux_in_air_", 
                i, ".Rdata")))
            
        }
    }
    
    return(mod.out)
}

model.air_pressure <- function(dat.train, n.beta, path.out, resids = F, 
    parallel = F, n.cores = NULL, day.window = 5, seed = 1347) {
    
    set.seed(seed)
    if (!dir.exists(path.out)) 
        dir.create(path.out, recursive = T)
    
    # The model we're going to use
    model.train <- function(dat.subset, n.beta, resids = resids) {
        
        # mod.doy <- lm(air_pressure ~ as.factor(hour)*(air_pressure.day +
        # lag.air_pressure + next.air_pressure)-as.factor(hour)-1,
        # data=dat.subset) ###
        mod.doy <- lm(air_pressure ~ as.factor(hour) * (air_pressure.day + 
            lag.air_pressure + next.air_pressure) - as.factor(hour) - 1 - 
            air_pressure.day - lag.air_pressure - next.air_pressure, data = dat.subset)  ###
        
        # If we can't estimate the covariance matrix, double our data and try
        # again NOTE: THIS IS NOT A GOOD PERMANENT FIX!!
        if (is.na(summary(mod.doy)$adj.r.squared)) {
            stop(paste0("Can not estimate covariance matrix for day of year: ", 
                unique(dat.subset$doy), ";  Increase day.window and try again"))
            # dat.subset <- rbind(dat.subset, dat.subset) mod.doy <-
            # lm(air_pressure ~ as.factor(hour)*(air_pressure.day +
            # lag.air_pressure +
            # next.air_pressure)-as.factor(hour)-1-air_pressure.day -
            # lag.air_pressure - next.air_pressure, data=dat.subset) ###
        }
        
        # Generate a bunch of random coefficients that we can pull from without
        # needing to do this step every day
        mod.coef <- coef(mod.doy)
        mod.cov <- vcov(mod.doy)
        piv <- as.numeric(which(!is.na(mod.coef)))
        Rbeta <- MASS::mvrnorm(n = n.beta, mod.coef[piv], mod.cov)
        
        list.out <- list(model = mod.doy, betas = Rbeta)
        # Model residuals as a function of hour so we can increase our
        # uncertainty
        if (resids == T) {
            dat.subset[!is.na(dat.subset$lag.air_pressure) & !is.na(dat.subset$next.air_pressure), 
                "resid"] <- resid(mod.doy)
            resid.model <- lm(resid ~ as.factor(hour) * air_pressure.day - 
                1, data = dat.subset[, ])
            res.coef <- coef(resid.model)
            res.cov <- vcov(resid.model)
            res.piv <- as.numeric(which(!is.na(res.coef)))
            
            beta.resid <- MASS::mvrnorm(n = n.beta, res.coef[res.piv], 
                res.cov)
            
            list.out[["model.resid"]] <- resid.model
            list.out[["betas.resid"]] <- beta.resid
        }
        
        # bernoulli distribution for precipitation megan kirkmeyer young paper
        # on evaluation of probabalistic downscaling, look at likelihood, it
        # would be nice to show the probablistic nature of temporal
        # downscaling, talk about the range of things that could be occuring at
        # that time.  lOok at threshold excedence, linear regression optimizes
        # your R squared so don't do that, plot the f value, the ratio of
        # variance, f values should show that you reproduce the variance. it
        # really highlights keeping the noise in there and just saying there is
        # a 1 to 1 relationship log likelihood ratios for each month/season,
        # month by month looks good too
        return(list.out)
    }
    
    dat.list <- list()
    mod.out <- list()
    
    # Make the data into a list Training the model on ax X-day window
    # around the actual DOY we're trying to model this helps avoid problems
    # with lack of data in small datasets like Ameriflux Default window is
    # 5 days (+/- 2)
    for (i in unique(dat.train$doy)) {
        if (i >= 365) {
            # Lump leap day in with non-leap Dec 31
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= 365 - day.window/2 | 
                dat.train$doy <= day.window/2, ]
        } else if (i == 1) {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy <= i + day.window/2 | 
                dat.train$doy >= 365 - day.window/2, ]
        } else {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= i - day.window/2 & 
                dat.train$doy <= i + day.window/2, ]
        }
    }
    
    # Do the computation and save a list Final list will have 2 layers per
    # DOY: the model, and a bunch of simulated betas
    if (parallel == T) {
        warning("Running model calculation in parallel.  This WILL crash if you do not have access to a LOT of memory!")
        library(parallel)
        
        mod.out <- parallel::mclapply(dat.list, model.train, mc.cores = n.cores, 
            n.beta = n.beta, resids = resids)
        
        # Use a loop to sace each day of year independently
        for (i in names(mod.out)) {
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_air_pressure_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[[i]][["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[[i]][["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[[i]][["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out[[i]][["model"]]
            save(mod.save, file = file.path(path.out, paste0("model_air_pressure_", 
                i, ".Rdata")))
        }
        
    } else {
        for (i in names(dat.list)) {
            mod.out <- model.train(dat.subset = dat.list[[i]], n.beta = n.beta, 
                resids = resids)
            
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_air_pressure_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out$mode
            save(mod.save, file = file.path(path.out, paste0("model_air_pressure_", 
                i, ".Rdata")))
            
        }
    }
    
    return(mod.out)
}

model.wind_speed <- function(dat.train, n.beta, path.out, resids = F, parallel = F, 
    n.cores = NULL, day.window = 5, seed = 708) {
    
    set.seed(seed)
    if (!dir.exists(path.out)) 
        dir.create(path.out, recursive = T)
    
    # The model we're going to use
    model.train <- function(dat.subset, n.beta, resids = resids) {
        
        # mod.doy <- lm(log(wind_speed) ~
        # as.factor(hour)*log(wind_speed.day)*(log(lag.wind_speed) +
        # log(next.wind_speed) + air_pressure.day + air_temperature_min.day +
        # air_temperature_max.day)-as.factor(hour)-1 - air_pressure.day -
        # air_temperature_min.day - air_temperature_max.day -
        # log(wind_speed.day)*air_pressure.day -
        # log(wind_speed.day)*air_temperature_min.day-
        # log(wind_speed.day)*air_temperature_max.day -
        # as.factor(hour)*air_temperature_min.day -
        # as.factor(hour)*air_temperature_max.day -
        # as.factor(hour)*air_pressure.day, data=dat.subset) ### mod.doy <-
        # lm(log(wind_speed) ~ as.factor(hour)*wind_speed.day*(lag.wind_speed +
        # next.wind_speed)-as.factor(hour)-1 - wind_speed.day - lag.wind_speed
        # - next.wind_speed - wind_speed.day*lag.wind_speed -
        # wind_speed.day*next.wind_speed, data=dat.subset) ###
        mod.doy <- lm(sqrt(wind_speed) ~ as.factor(hour) * wind_speed.day * 
            (lag.wind_speed + next.wind_speed) - as.factor(hour) - 1 - 
            wind_speed.day - lag.wind_speed - next.wind_speed - wind_speed.day * 
            lag.wind_speed - wind_speed.day * next.wind_speed, data = dat.subset)  ###
        
        # If we can't estimate the covariance matrix, stop & increase the
        # moving window
        if (is.na(summary(mod.doy)$adj.r.squared)) {
            stop(paste0("Can not estimate covariance matrix for day of year: ", 
                unique(dat.subset$doy), ";  Increase day.window and try again"))
        }
        
        # Generate a bunch of random coefficients that we can pull from without
        # needing to do this step every day
        mod.coef <- coef(mod.doy)
        mod.cov <- vcov(mod.doy)
        piv <- as.numeric(which(!is.na(mod.coef)))
        Rbeta <- MASS::mvrnorm(n = n.beta, mod.coef[piv], mod.cov)
        
        list.out <- list(model = mod.doy, betas = Rbeta)
        # Model residuals as a function of hour so we can increase our
        # uncertainty
        if (resids == T) {
            dat.subset[!is.na(dat.subset$lag.wind_speed) & !is.na(dat.subset$next.wind_speed), 
                "resid"] <- resid(mod.doy)
            resid.model <- lm(resid ~ as.factor(hour) * wind_speed.day - 
                1, data = dat.subset[, ])
            res.coef <- coef(resid.model)
            res.cov <- vcov(resid.model)
            res.piv <- as.numeric(which(!is.na(res.coef)))
            
            beta.resid <- MASS::mvrnorm(n = n.beta, res.coef[res.piv], 
                res.cov)
            
            list.out[["model.resid"]] <- resid.model
            list.out[["betas.resid"]] <- beta.resid
        }
        
        return(list.out)
    }
    
    dat.list <- list()
    mod.out <- list()
    
    # Make the data into a list Training the model on ax X-day window
    # around the actual DOY we're trying to model this helps avoid problems
    # with lack of data in small datasets like Ameriflux Default window is
    # 5 days (+/- 2)
    for (i in unique(dat.train$doy)) {
        if (i >= 365) {
            # Lump leap day in with non-leap Dec 31
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= 365 - day.window/2 | 
                dat.train$doy <= day.window/2, ]
        } else if (i == 1) {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy <= i + day.window/2 | 
                dat.train$doy >= 365 - day.window/2, ]
        } else {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= i - day.window/2 & 
                dat.train$doy <= i + day.window/2, ]
        }
    }
    
    # Do the computation and save a list Final list will have 2 layers per
    # DOY: the model, and a bunch of simulated betas
    if (parallel == T) {
        warning("Running model calculation in parallel.  This WILL crash if you do not have access to a LOT of memory!")
        library(parallel)
        mod.out <- parallel::mclapply(dat.list, model.train, mc.cores = n.cores, 
            n.beta = n.beta, resids = resids)
        
        # Use a loop to sace each day of year independently
        for (i in names(mod.out)) {
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_wind_speed_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[[i]][["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[[i]][["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[[i]][["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out[[i]][["model"]]
            save(mod.save, file = file.path(path.out, paste0("model_wind_speed_", 
                i, ".Rdata")))
        }
        
    } else {
        for (i in names(dat.list)) {
            mod.out <- model.train(dat.subset = dat.list[[i]], n.beta = n.beta, 
                resids = resids)
            
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_wind_speed_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out$mode
            save(mod.save, file = file.path(path.out, paste0("model_wind_speed_", 
                i, ".Rdata")))
            
        }
    }
    
    return(mod.out)
}

model.precipitation_flux <- function(dat.train, n.beta, path.out, resids = F, 
    parallel = F, n.cores = NULL, day.window = 5, seed = 1562) {
    
    # library(fitdistrplus)
    set.seed(seed)
    if (!dir.exists(path.out)) 
        dir.create(path.out, recursive = T)
    
    # The model we're going to use
    model.train <- function(dat.subset, n.beta, resids = resids) {
        
        # Precip needs to be a bit different.  We're going to calculate the
        # fraction of precip occuring in each hour we're going to estimate the
        # probability distribution of rain occuring in a given hour
        dat.subset$rain.prop <- dat.subset$precipitation_flux/(dat.subset$precipitation_flux.day * 
            24)
        mod.doy <- lm(rain.prop ~ as.factor(hour) * precipitation_flux.day - 
            1 - as.factor(hour) - precipitation_flux.day, data = dat.subset)
        
        # If we can't estimate the covariance matrix, increase the moving
        # window
        if (is.na(summary(mod.doy)$adj.r.squared)) {
            stop(paste0("Can not estimate covariance matrix for day of year: ", 
                unique(dat.subset$doy), ";  Increase day.window and try again"))
            # dat.subset <- rbind(dat.subset, dat.subset) mod.doy <- lm(rain.prop ~
            # as.factor(hour)*precipitation_flux.day-1 -
            # as.factor(hour)-precipitation_flux.day, data=dat.subset)
        }
        
        # Generate a bunch of random coefficients that we can pull from without
        # needing to do this step every day
        mod.coef <- coef(mod.doy)
        mod.cov <- vcov(mod.doy)
        piv <- as.numeric(which(!is.na(mod.coef)))
        Rbeta <- MASS::mvrnorm(n = n.beta, mod.coef[piv], mod.cov)
        
        list.out <- list(model = mod.doy, betas = Rbeta)
        # Model residuals as a function of hour so we can increase our
        # uncertainty
        if (resids == T) {
            # dat.subset[!is.na(dat.subset$lag.precipitation_flux) &
            # !is.na(dat.subset$next.precipitation_flux),'resid'] <- resid(mod.doy)
            dat.subset[, "resid"] <- resid(mod.doy)
            resid.model <- lm(resid ~ as.factor(hour) * precipitation_flux.day - 
                1, data = dat.subset[, ])
            res.coef <- coef(resid.model)
            res.cov <- vcov(resid.model)
            res.piv <- as.numeric(which(!is.na(res.coef)))
            
            beta.resid <- MASS::mvrnorm(n = n.beta, res.coef[res.piv], 
                res.cov)
            
            list.out[["model.resid"]] <- resid.model
            list.out[["betas.resid"]] <- beta.resid
        }
        
        return(list.out)
    }
    
    dat.list <- list()
    mod.out <- list()
    
    # Make the data into a list Training the model on ax X-day window
    # around the actual DOY we're trying to model this helps avoid problems
    # with lack of data in small datasets like Ameriflux Default window is
    # 5 days (+/- 2)
    for (i in unique(dat.train$doy)) {
        if (i >= 365) {
            # Lump leap day in with non-leap Dec 31
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= 365 - day.window/2 | 
                dat.train$doy <= day.window/2, ]
        } else if (i == 1) {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy <= i + day.window/2 | 
                dat.train$doy >= 365 - day.window/2, ]
        } else {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= i - day.window/2 & 
                dat.train$doy <= i + day.window/2, ]
        }
    }
    
    # Do the computation and save a list Final list will have 2 layers per
    # DOY: the model, and a bunch of simulated betas
    if (parallel == T) {
        warning("Running model calculation in parallel.  This WILL crash if you do not have access to a LOT of memory!")
        library(parallel)
        mod.out <- parallel::mclapply(dat.list, model.train, mc.cores = n.cores, 
            n.beta = n.beta, resids = resids)
        
        # Use a loop to sace each day of year independently
        for (i in names(mod.out)) {
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_precipitation_flux_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[[i]][["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[[i]][["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[[i]][["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out[[i]][["model"]]
            save(mod.save, file = file.path(path.out, paste0("model_precipitation_flux_", 
                i, ".Rdata")))
        }
        
    } else {
        for (i in names(dat.list)) {
            mod.out <- model.train(dat.subset = dat.list[[i]], n.beta = n.beta, 
                resids = resids)
            
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_precipitation_flux_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out$mode
            save(mod.save, file = file.path(path.out, paste0("model_precipitation_flux_", 
                i, ".Rdata")))
            
        }
    }
    
    return(mod.out)
}

model.specific_humidity <- function(dat.train, n.beta, path.out, resids = F, 
    parallel = F, n.cores = NULL, day.window = 5, seed = 1009) {
    
    set.seed(seed)
    if (!dir.exists(path.out)) 
        dir.create(path.out, recursive = T)
    
    # The model we're going to use
    model.train <- function(dat.subset, n.beta, resids = resids) {
        
        # mod.doy <- lm(log(specific_humidity) ~
        # as.factor(hour)*log(specific_humidity.day)*(log(lag.specific_humidity)
        # + log(next.specific_humidity) + precipitation_flux.day +
        # air_temperature_min.day + air_temperature_max.day)-as.factor(hour)-1
        # - precipitation_flux.day - air_temperature_min.day -
        # air_temperature_max.day -
        # log(specific_humidity.day)*precipitation_flux.day -
        # log(specific_humidity.day)*air_temperature_min.day-
        # log(specific_humidity.day)*air_temperature_max.day -
        # as.factor(hour)*air_temperature_min.day -
        # as.factor(hour)*air_temperature_max.day -
        # as.factor(hour)*precipitation_flux.day, data=dat.subset) ### mod.doy
        # <- lm(log(specific_humidity) ~
        # as.factor(hour)*specific_humidity.day*(lag.specific_humidity +
        # next.specific_humidity)-as.factor(hour)-1 - specific_humidity.day -
        # lag.specific_humidity - next.specific_humidity -
        # specific_humidity.day*lag.specific_humidity -
        # specific_humidity.day*next.specific_humidity, data=dat.subset) ###
        mod.doy <- lm(log(specific_humidity) ~ as.factor(hour) * specific_humidity.day * 
            (lag.specific_humidity + next.specific_humidity + air_temperature_max.day) - 
            as.factor(hour) - 1 - air_temperature_max.day, data = dat.subset)  ###
        # mod.doy <- glm(specific_humidity ~
        # as.factor(hour)*specific_humidity.day*(lag.specific_humidity +
        # next.specific_humidity)-as.factor(hour)-1 - specific_humidity.day -
        # lag.specific_humidity - next.specific_humidity -
        # specific_humidity.day*lag.specific_humidity -
        # specific_humidity.day*next.specific_humidity, data=dat.subset,
        # family='quasibinomial') ###
        
        # If we can't estimate the covariance matrix, stop & increasing the
        # moving window
        if (is.na(summary(mod.doy)$adj.r.squared)) {
            stop(paste0("Can not estimate covariance matrix for day of year: ", 
                unique(dat.subset$doy), ";  Increase day.window and try again"))
            # dat.subset <- rbind(dat.subset, dat.subset) mod.doy <-
            # lm(log(specific_humidity) ~
            # as.factor(hour)*specific_humidity.day*(lag.specific_humidity +
            # next.specific_humidity + air_temperature_max.day)-as.factor(hour)-1 -
            # air_temperature_max.day, data=dat.subset) ###
        }
        
        
        # Generate a bunch of random coefficients that we can pull from without
        # needing to do this step every day
        mod.coef <- coef(mod.doy)
        mod.cov <- vcov(mod.doy)
        piv <- as.numeric(which(!is.na(mod.coef)))
        Rbeta <- MASS::mvrnorm(n = n.beta, mod.coef[piv], mod.cov)
        
        list.out <- list(model = mod.doy, betas = Rbeta)
        # Model residuals as a function of hour so we can increase our
        # uncertainty
        if (resids == T) {
            dat.subset[!is.na(dat.subset$lag.specific_humidity) & !is.na(dat.subset$next.specific_humidity), 
                "resid"] <- resid(mod.doy)
            resid.model <- lm(resid ~ as.factor(hour) * specific_humidity.day - 
                1, data = dat.subset[, ])
            res.coef <- coef(resid.model)
            res.cov <- vcov(resid.model)
            res.piv <- as.numeric(which(!is.na(res.coef)))
            
            beta.resid <- MASS::mvrnorm(n = n.beta, res.coef[res.piv], 
                res.cov)
            
            list.out[["model.resid"]] <- resid.model
            list.out[["betas.resid"]] <- beta.resid
        }
        
        return(list.out)
    }
    
    dat.list <- list()
    mod.out <- list()
    
    # Make the data into a list Training the model on ax X-day window
    # around the actual DOY we're trying to model this helps avoid problems
    # with lack of data in small datasets like Ameriflux Default window is
    # 5 days (+/- 2)
    for (i in unique(dat.train$doy)) {
        if (i >= 365) {
            # Lump leap day in with non-leap Dec 31
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= 365 - day.window/2 | 
                dat.train$doy <= day.window/2, ]
        } else if (i == 1) {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy <= i + day.window/2 | 
                dat.train$doy >= 365 - day.window/2, ]
        } else {
            dat.list[[paste(i)]] <- dat.train[dat.train$doy >= i - day.window/2 & 
                dat.train$doy <= i + day.window/2, ]
        }
    }
    
    # Do the computation and save a list Final list will have 2 layers per
    # DOY: the model, and a bunch of simulated betas
    if (parallel == T) {
        warning("Running model calculation in parallel.  This WILL crash if you do not have access to a LOT of memory!")
        library(parallel)
        mod.out <- parallel::mclapply(dat.list, model.train, mc.cores = n.cores, 
            n.beta = n.beta, resids = resids)
        
        # Use a loop to sace each day of year independently
        for (i in names(mod.out)) {
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_specific_humidity_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[[i]][["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[[i]][["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[[i]][["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out[[i]][["model"]]
            save(mod.save, file = file.path(path.out, paste0("model_specific_humidity_", 
                i, ".Rdata")))
        }
        
    } else {
        for (i in names(dat.list)) {
            mod.out <- model.train(dat.subset = dat.list[[i]], n.beta = n.beta, 
                resids = resids)
            
            # Save the betas as .nc
            outfile <- file.path(path.out, paste0("betas_specific_humidity_", 
                i, ".nc"))
            dimY <- ncdf4::ncdim_def(paste0("coeffs_", i), units = "unitless", 
                longname = "model.out coefficients", vals = 1:ncol(mod.out[["betas"]]))
            dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
                vals = 1:nrow(mod.out[["betas"]]))
            var.list <- ncdf4::ncvar_def(i, units = "coefficients", dim = list(dimX, 
                dimY), longname = paste0("day ", i, " model.out coefficients"))
            nc <- ncdf4::nc_create(outfile, var.list)
            ncdf4::ncvar_put(nc, var.list, mod.out[["betas"]])
            ncdf4::nc_close(nc)
            
            # Save the model as a .Rdata
            mod.save <- mod.out$mode
            save(mod.save, file = file.path(path.out, paste0("model_specific_humidity_", 
                i, ".Rdata")))
            
        }
    }
    
    return(mod.out)
}

predict.met <- function(newdata, model.predict, Rbeta, resid.err = F, model.resid = NULL, 
    Rbeta.resid = NULL, n.ens) {
    # set.seed(9321)
    err.resid <- 0  # dummy residual error term; if we want to add residual error, we're modeling it by hour
    
    mod.terms <- terms(model.predict)
    mod.coef <- coef(model.predict)
    mod.cov <- vcov(model.predict)
    mod.resid <- resid(model.predict)
    piv <- as.numeric(which(!is.na(mod.coef)))
    
    m <- model.frame(mod.terms, newdata, xlev = model.predict$xlevels)
    Xp <- model.matrix(mod.terms, m, contrasts.arg = model.predict$contrasts)
    
    if (resid.err == T) {
        newdata$resid <- 99999
        resid.terms <- terms(model.resid)
        resid.coef <- coef(model.resid)
        resid.cov <- vcov(model.resid)
        resid.resid <- resid(model.resid)
        resid.piv <- as.numeric(which(!is.na(resid.coef)))
        
        m2 <- model.frame(resid.terms, newdata, xlev = model.resid$xlevels)
        Xp.res <- model.matrix(resid.terms, m2, contrasts.arg = model.resid$contrasts)
        
        err.resid <- Xp.res[, resid.piv] %*% t(Rbeta.resid)
    }
    
    dat.sim <- Xp[, piv] %*% t(Rbeta) + err.resid
    
    return(dat.sim)
    
}

save.betas <- function(model.out, betas, outfile) {
    # Function to save betas as a .nc file model.out = the model output
    # list betas = the name of the layer of betas to save (e.g. 'betas', or
    # 'betas.resid')
    
    # Save a vector of the dimnames names.coefs <-
    # dimnames(model.out[[1]][[betas]])[[2]]
    
    var.list <- list()
    for (v in names(model.out)) {
        # Note: Need a separate list of coefficients for each variable to make
        # my life easier if the is surface_downwelling_shortwave_flux_in_air
        # which has varying predictors by day
        dimY <- ncdf4::ncdim_def(paste0("coeffs_", v), units = "unitless", 
            longname = "model.out coefficients", vals = 1:ncol(model.out[[v]][[betas]]))
        dimX <- ncdf4::ncdim_def("random", units = "unitless", longname = "random betas", 
            vals = 1:nrow(model.out[[v]][[betas]]))
        
        var.list[[v]] <- ncdf4::ncvar_def(v, units = "coefficients", dim = list(dimX, 
            dimY), longname = paste0("day ", v, " model.out coefficients"))
    }
    
    # dim.string <- ncdf4::ncdim_def('names', '',
    # 1:max(nchar(names.coefs)), create_dimvar=FALSE)
    # var.list[['names.coefs']] <- ncdf4::ncvar_def('names.coefs',
    # units='', dim=list(dim.string, dimY), longname='model coefficient
    # names', prec='char')
    
    nc <- ncdf4::nc_create(outfile, var.list)
    # ncdf4::ncvar_put(nc, var.list$names.coefs, names.coefs)
    for (v in names(model.out)) {
        ncdf4::ncvar_put(nc, var.list[[v]], model.out[[v]][[betas]])
    }
    ncdf4::nc_close(nc)
    
    
}

save.model <- function(model.out, model, outfile) {
    # Function to save linear models as a .Rdata file model.out = the model
    # output list model = the name of the layer of betas to save (e.g.
    # 'model', or 'model.resid')
    
    mod.list <- list()
    for (v in names(model.out)) {
        mod.list[[v]] <- model.out[[v]][[model]]
    }
    
    save(mod.list, file = outfile)
}
