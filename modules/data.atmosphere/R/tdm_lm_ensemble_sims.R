##' Linear Regression Ensemble Simulation
##' Met downscaling function that predicts ensembles of downscaled meteorology
# -----------------------------------
# Description
# -----------------------------------
##' @title lm_ensemble_sims
##' @family tdm - Temporally Downscale Meteorology
##' @author Christy Rollinson, James Simkins
##' @description This function does the heavy lifting in the final
##'              function of the tdm workflow titled predict_subdaily_met(). It uses a linear
##'              regression approach by generating the hourly values from the coarse data of
##'              the file the user selects to downscale based on the hourly models and betas
##'              generated by gen.subdaily.models().
# -----------------------------------
# Parameters
# -----------------------------------
##' @param dat.mod - dataframe to be predicted at the time step of the training data
##' @param n.ens - number of hourly ensemble members to generate
##' @param path.model - path to where the training model & betas is stored
##' @param direction.filter - Whether the model will be filtered backward or forward in time. options = c("backward", "forward")
##'                           (PalEON will go backward, anybody interested in the future will go forward)
##' @param lags.list - optional list form of lags.init, with one entry for each unique `ens.day` in dat.mod
##' @param lags.init - a data frame of initialization parameters to match the data in dat.mod
##' @param dat.train - the training data used to fit the model; needed for night/day in
##'                    surface_downwelling_shortwave_flux_in_air
##' @param precip.distribution - a list with 2 sub-lists containing the number of observations with precip in the training data per day &
##'                              the hour of max rain in the training data.  This will be used to help solve the "constant drizzle" problem
##' @param force.sanity - (logical) do we force the data to meet sanity checks?
##' @param sanity.tries - how many time should we try to predict a reasonable value before giving up?  We don't want to end up in an infinite loop
##' @param sanity.sd - how many standard deviations from the mean should be used to determine sane outliers (default 6)
##' @param seed - (optional) set the seed manually to allow reproducible results
##' @param print.progress - if TRUE will print progress bar
##' @export
# -----------------------------------
#----------------------------------------------------------------------
# Begin Function
#----------------------------------------------------------------------

lm_ensemble_sims <- function(dat.mod, n.ens, path.model, direction.filter, lags.list = NULL,
                             lags.init = NULL, dat.train, precip.distribution,
                             force.sanity=TRUE, sanity.tries=25, sanity.sd=6,
                             seed=Sys.time(), print.progress=FALSE) {

  # Set our random seed
  set.seed(seed)

  # Just in case we have a capitalization or singular/plural issue
  if(direction.filter %in% toupper( c("backward", "backwards"))) direction.filter="backward"

  # Setting our our time indexes
  if(direction.filter=="backward"){
    days.sim <- max(dat.mod$sim.day):min(dat.mod$sim.day)
    lag.time <- min(dat.mod$hour)
  } else {
    days.sim <- min(dat.mod$sim.day):max(dat.mod$sim.day)
    lag.time <- max(dat.mod$hour)
  }

  # Declare the variables of interest that will be called in the
  # overarching loop
  vars.list <- c("surface_downwelling_shortwave_flux_in_air", "air_temperature",
                 "precipitation_flux", "surface_downwelling_longwave_flux_in_air",
                 "air_pressure", "specific_humidity", "wind_speed")

  # Data info that will be used to help organize dataframe for
  # downscaling
  dat.info <- c("sim.day", "year", "doy", "hour", "air_temperature_max.day",
                "air_temperature_min.day", "precipitation_flux.day", "surface_downwelling_shortwave_flux_in_air.day",
                "surface_downwelling_longwave_flux_in_air.day", "air_pressure.day",
                "specific_humidity.day", "wind_speed.day", "next.air_temperature_max",
                "next.air_temperature_min", "next.precipitation_flux", "next.surface_downwelling_shortwave_flux_in_air",
                "next.surface_downwelling_longwave_flux_in_air", "next.air_pressure",
                "next.specific_humidity", "next.wind_speed")

  # # Set progress bar
  if(print.progress==TRUE){
    pb.index <- 1
    pb <- utils::txtProgressBar(min = 1, max = length(vars.list)*length(days.sim), style = 3)
    utils::setTxtProgressBar(pb, pb.index)
  }

  # Figure out if we need to extract the approrpiate
  if (is.null(lags.list) & is.null(lags.init)) {
   PEcAn.logger::logger.error("lags.init & lags.list are NULL, this is a required argument")
  }
  if (is.null(lags.init)) {
    lags.init <- lags.list[[unique(dat.mod$ens.day)]]
  }


  # Set up the ensemble members in a list so the uncertainty can be
  # propogated
  dat.sim <- list()

 # ------ Beginning of Downscaling For Loop

  for (v in vars.list) {
    # Initalize our ouroutput
    dat.sim[[v]] <- array(dim=c(nrow(dat.mod), n.ens))

    # create column propagation list and betas progagation list
    cols.list <- array(dim=c(length(days.sim), n.ens)) # An array with number of days x number of ensembles
    rows.beta <- vector(length=n.ens) # A vector that ends up being the length of the number of our days

    # This gives us a
    for (i in seq_len(length(days.sim))) {
      cols.tem <- sample(1:n.ens, n.ens, replace = TRUE)
      cols.list[i,] <- cols.tem
    }

    # Read in the first linear regression model
    first_model <- ncdf4::nc_open(paste0(path.model, "/", v, "/betas_", v, "_1.nc"))
    # first_beta <- assign(paste0("betas.", v, "_1"), first_model) # does below need to be first_beta?
    n.beta <- first_model$var[[1]]$dim[[1]]$len # Number of rows; should be same for all
    ncdf4::nc_close(first_model)

    # Create beta list so each ensemble for each variable pulls the same
    # betas
    # for (i in seq_len(length(days.sim))) {
    #   betas.tem <- sample(1:(n.beta-n.ens), 1, replace = TRUE)
    #   rows.beta[i] <- betas.tem
    # }
    # rows.beta <- as.numeric(rows.beta)

    # fill our dat.sim list
    dat.sim[[v]] <- data.frame(array(dim = c(nrow(dat.mod), n.ens)))

    # --------------------------------
    # Looping through time
    # --------------------------------
    # Setting our our time indexes
    for (i in 1:length(days.sim)) {
      day.now <- unique(dat.mod[dat.mod$sim.day == days.sim[i], "doy"])
      rows.now <- which(dat.mod$sim.day == days.sim[i])

      # shortwave is different because we only want to model daylight
      if (v == "surface_downwelling_shortwave_flux_in_air") {
        # Finding which days have measurable light
        thresh.swdown <- stats::quantile(dat.train$surface_downwelling_shortwave_flux_in_air[dat.train$surface_downwelling_shortwave_flux_in_air > 0], 0.05)


        hrs.day <- unique(dat.train$time[dat.train$time$DOY == day.now &
                                         dat.train$surface_downwelling_shortwave_flux_in_air > thresh.swdown,
                                         "Hour"])

        rows.mod <- which(dat.mod$sim.day == days.sim[i] & dat.mod$hour %in% hrs.day)
        dat.temp <- dat.mod[rows.mod, dat.info]

        # dat.temp <- merge(dat.temp, data.frame(ens=paste0("X", 1:n.ens)))
        if (i == 1) {
          sim.lag <- utils::stack(lags.init[[v]])
          names(sim.lag) <- c(paste0("lag.", v), "ens")

        } else {
          sim.lag <- utils::stack(data.frame(array(0,dim = c(1, ncol(dat.sim[[v]])))))
          names(sim.lag) <- c(paste0("lag.", v), "ens")
        }
        dat.temp <- merge(dat.temp, sim.lag, all.x = TRUE)

      } else if (v == "air_temperature") {
        dat.temp <- dat.mod[rows.now, dat.info]

        # Set up the lags
        if (i == 1) { # First time through, so pull from our inital lags
          sim.lag <- utils::stack(lags.init$air_temperature)
          names(sim.lag) <- c("lag.air_temperature", "ens")

          sim.lag$lag.air_temperature_min <- utils::stack(lags.init$air_temperature_min)[,1]
          sim.lag$lag.air_temperature_max <- utils::stack(lags.init$air_temperature_max)[,1]
        } else {
          sim.lag <- utils::stack(data.frame(array(dat.sim[["air_temperature"]][dat.mod$sim.day == (days.sim[i-1]) &
                                                                           dat.mod$hour == lag.time, ],
                                            dim = c(1, ncol(dat.sim$air_temperature)))))
          names(sim.lag) <- c("lag.air_temperature", "ens")
          sim.lag$lag.air_temperature_min <- utils::stack(apply(data.frame(dat.sim[["air_temperature"]][dat.mod$sim.day == days.sim[i-1], ]), 2, min))[, 1]
          sim.lag$lag.air_temperature_max <- utils::stack(apply(data.frame(dat.sim[["air_temperature"]][dat.mod$sim.day == days.sim[i-1], ]), 2, max))[, 1]
        }
        dat.temp <- merge(dat.temp, sim.lag, all.x = TRUE)
      } else if (v == "precipitation_flux") {
        dat.temp <- dat.mod[rows.now, dat.info]

        dat.temp[,v] <- 99999
        dat.temp$rain.prop <- 99999

        day.now <- unique(dat.temp$doy)

        # Set up the lags This is repeated differently because Precipitation
        # dat.temp is merged
        if (i == 1) {
          sim.lag <- utils::stack(lags.init[[v]])
          names(sim.lag) <- c(paste0("lag.", v), "ens")

        } else {
          sim.lag <- utils::stack(data.frame(array(dat.sim[[v]][dat.mod$sim.day == days.sim[i-1] &
                                                           dat.mod$hour == lag.time, ],
                                            dim = c(1, ncol(dat.sim[[v]])))))
          names(sim.lag) <- c(paste0("lag.", v), "ens")
        }
        dat.temp <- merge(dat.temp, sim.lag, all.x = TRUE)

        # End Precipitation Flux specifics
      } else {
        dat.temp <- dat.mod[rows.now, dat.info]

        if (i == 1) {
          sim.lag <- utils::stack(lags.init[[v]])
          names(sim.lag) <- c(paste0("lag.", v), "ens")

        } else {
          sim.lag <- utils::stack(data.frame(array(dat.sim[[v]][dat.mod$sim.day == days.sim[i-1] &
                                                           dat.mod$hour == lag.time, ],
                                            dim = c(1, ncol(dat.sim[[v]])))))
          names(sim.lag) <- c(paste0("lag.", v), "ens")
        }
        dat.temp <- merge(dat.temp, sim.lag, all.x = TRUE)
      } # End special formatting

      # Create dummy value
      dat.temp[,v] <- 99999

      # Creating some necessary dummy variable names
      vars.sqrt <- c("surface_downwelling_longwave_flux_in_air", "wind_speed")
      vars.log <- c("specific_humidity")
      if (v %in% vars.sqrt) {
      	dat.temp[,paste0("sqrt(",v,")")] <- sqrt(dat.temp[,v])
      } else if (v %in% vars.log) {
      	dat.temp[,paste0("log(",v,")")] <- log(dat.temp[,v])
      }

      # Load the saved model
      model.file <- file.path(path.model, v, paste0("model_", v, "_", day.now,
                                                    ".Rdata"))
      if(file.exists(model.file)) {
        env = new.env()
        load(model.file, envir = env)
        mod.save <- env$mod.save
      }

      # Pull coefficients (betas) from our saved matrix

      # for (i in seq_len(length(days.sim))) {
        # betas.tem <- sample(1:(n.beta-n.ens), 1, replace = TRUE)
        # rows.beta[i] <- betas.tem
      # }
      # rows.beta <- as.numeric(rows.beta)

      # n.new <- ifelse(n.ens==1, 10, n.ens) # If we're not creating an ensemble, we'll add a mean step to remove chance of odd values
      n.new <- n.ens
      cols.redo <- 1:n.new
      sane.attempt=0
      betas_nc <- ncdf4::nc_open(file.path(path.model, v, paste0("betas_", v, "_", day.now, ".nc")))
      col.beta <- betas_nc$var[[1]]$dim[[2]]$len # number of coefficients
      while(n.new>0 & sane.attempt <= sanity.tries){

        if(n.ens==1){
          Rbeta <- matrix(mod.save$coef, ncol=col.beta)
        } else {
          betas.tem <- sample(1:max((n.beta-n.new), 1), 1, replace = TRUE)
          Rbeta <- matrix(ncdf4::ncvar_get(betas_nc, paste(day.now), c(betas.tem,1), c(n.new,col.beta)), ncol = col.beta)
        }


        if(ncol(Rbeta)!=col.beta) Rbeta <- t(Rbeta)

        # If we're starting from scratch, set up the prediction matrix
        if(sane.attempt==0){
          dat.pred <- matrix(nrow=nrow(dat.temp), ncol=n.ens)
        }

        # if(n.ens==1){
        #   dat.dum <- subdaily_pred(newdata = dat.temp, model.predict = mod.save,
        #                            Rbeta = Rbeta, resid.err = FALSE, model.resid = NULL, Rbeta.resid = NULL,
        #                            n.ens = n.new)
        #   dat.pred[,1] <- apply(dat.dum, 1, mean)
        # } else {
          dat.pred[,cols.redo] <- subdaily_pred(newdata = dat.temp, model.predict = mod.save,
                                                Rbeta = Rbeta, resid.err = FALSE, model.resid = NULL, Rbeta.resid = NULL,
                                                n.ens = n.new)

        # }

        # Occasionally specific humidty may go serioulsy off the rails
        if(v=="specific_humidity" & (max(dat.pred)>log(40e-3) | min(dat.pred)<log(1e-6))){
          dat.pred[dat.pred>log(40e-3)] <- log(40e-3)
          dat.pred[dat.pred<log( 1e-6)] <- log(1e-6)
        }

        # Precipitation Re-distribute negative probabilities -- add randomly to
        # make more peaky If there's no rain on this day, skip the
        # re-proportioning
        if (v == "precipitation_flux") {

          # if(n.ens == 1) next
          cols.check <- cols.redo
          if (max(dat.pred[,cols.check]) > 0) {
            tmp <- 1:nrow(dat.pred)  # A dummy vector of the
            for (j in cols.check) {
              if (min(dat.pred[, j]) >= 0) next # skip if no negative rain to redistribute
              rows.neg <- which(dat.pred[, j] < 0)
              rows.add <- sample(tmp[!tmp %in% rows.neg], length(rows.neg),
                                 replace = TRUE)

              # Redistribute days with negative rain
              for (z in 1:length(rows.neg)) {
                dat.pred[rows.add[z], j] <- dat.pred[rows.add[z], j] - dat.pred[rows.neg[z], j]
                dat.pred[rows.neg[z], j] <- 0
              }
            } # j End loop

            # Make sure each day sums to 1
            # dat.pred[,cols.check] <- dat.pred[,cols.check]/colSums(data.frame(dat.pred[,cols.check]), na.rm=T)
            dat.pred[is.na(dat.pred)] <- 0
          } # End case of re-proportioning

          # Convert precip proportions into real units
          # Total Daily precip = precipitaiton_flux.day*24*60*60
          # precip.day <- dat.temp$precipitation_flux.day[1]*nrow(dat.temp)
          precip.day <- dat.temp$precipitation_flux.day[1]
          dat.pred[,cols.check] <- dat.pred[,cols.check] * precip.day
        } # End Precip re-propogation

        # -----
        # SANITY CHECKS!!!
        # -----
        # Here we'll also take into account the values from the past 2 weeks using a "six-sigma filter" per email with Ankur
        # -- this is apparently what they do with the flux data

        # vars.sqrt <- c("surface_downwelling_longwave_flux_in_air", "wind_speed")
        # vars.log <- c("specific_humidity")

        # Determine which ensemble members fail sanity checks
        #don't forget to check for transformed variables
        # vars.transform <- c("surface_downwelling_shortwave_flux_in_air", "specific_humidity", "surface_downwelling_longwave_flux_in_air", "wind_speed")
        # dat.sim[[v]][rows.now, j]
        if(i>14){
          if(direction.filter=="backward"){
            rows.filter <- which(dat.mod$sim.day >= days.sim[i] & dat.mod$sim.day <= days.sim[i]+14)
          } else {
            rows.filter <- which(dat.mod$sim.day <= days.sim[i] & dat.mod$sim.day >= days.sim[i]-14)
          }
          if(n.ens>1){
            dat.filter <- utils::stack(dat.sim[[v]][rows.filter,])[,1]
          } else {
            dat.filter <- dat.sim[[v]][rows.filter,]
          }


          filter.mean <- mean(dat.filter, na.rm=T)
          filter.sd   <- stats::sd(dat.filter, na.rm=T)
        } else {

          if(v %in% vars.sqrt){
            filter.mean <- mean(c(dat.pred^2, utils::stack(dat.sim[[v]])[,1]), na.rm=T)
            filter.sd   <- stats::sd(c(dat.pred^2, utils::stack(dat.sim[[v]])[,1]), na.rm=T)
          } else if(v %in% vars.log){
            filter.mean <- mean(c(exp(dat.pred), utils::stack(dat.sim[[v]])[,1]), na.rm=T)
            filter.sd   <- stats::sd(c(exp(dat.pred), utils::stack(dat.sim[[v]])[,1]), na.rm=T)
          } else {
            filter.mean <- mean(c(dat.pred, utils::stack(dat.sim[[v]])[,1]), na.rm=T)
            filter.sd   <- stats::sd(c(dat.pred, utils::stack(dat.sim[[v]])[,1]), na.rm=T)
          }
        }

        if(v %in% c("air_temperature", "air_temperature_maximum", "air_temperature_minimum")){
          # max air temp = 70 C; hottest temperature from sattellite; very ridiculous
          # min air temp = -95 C; colder than coldest natural temperature recorded in Antarctica

          tmax.ens <- max(dat.temp$air_temperature_max.day)
          tmin.ens <- min(dat.temp$air_temperature_min.day)

          # we'll allow some drift outside of what we have for our max/min, but not too much;
          # - right now general rule of thumb of 2 degrees leeway on the prescribed
          cols.redo <- which(apply(dat.pred, 2, function(x) min(x) < 184 | max(x) > 331 |
                                                           # min(x) < tmin.ens-2 | max(x) > tmax.ens+2 |
                                                           min(x) < filter.mean-sanity.sd*filter.sd | max(x) > filter.mean+sanity.sd*filter.sd
                                   ))
        }
        #"specific_humidity",
        if(v == "specific_humidity"){ #LOG!!
          # Based on google, it looks like values of 30 g/kg can occur in the tropics, so lets go above that
          # Also, the minimum humidity can't be 0 so lets just make it extremely dry; lets set this for 1 g/Mg
          cols.redo <- which(apply(dat.pred, 2, function(x) min(exp(x)) < 1e-6  | max(exp(x)) > 3.2e-2 |
                                                            min(exp(x)) < filter.mean-sanity.sd*filter.sd |
                                                            max(exp(x)) > filter.mean+sanity.sd*filter.sd
                                   ) )
        }
        #"surface_downwelling_shortwave_flux_in_air",
        if(v == "surface_downwelling_shortwave_flux_in_air"){
          # Based on something found from Columbia, average Radiative flux at ATM is 1360 W/m2, so for a daily average it should be less than this
          # Lets round 1360 and divide that by 2 (because it should be a daily average) and conservatively assume albedo of 20% (average value is more like 30)
          # Source http://eesc.columbia.edu/courses/ees/climate/lectures/radiation/
          dat.pred[dat.pred < 0] <- 0
          cols.redo <- which(apply(dat.pred, 2, function(x) max(x) > 1500 | min(x) < filter.mean-sanity.sd*filter.sd |
                                                            max(x) > filter.mean+sanity.sd*filter.sd
                                   ))
        }
        if(v == "air_pressure"){
          # According to wikipedia the highest barometric pressure ever recorded was 1085.7 hPa = 1085.7*100 Pa; Dead sea has average pressure of 1065 hPa
          #  - Lets round up to 1100 hPA
          # Also according to Wikipedia, the lowest non-tornadic pressure ever measured was 870 hPA
          cols.redo <- which(apply(dat.pred, 2, function(x) min(x) < 45000  | max(x) > 110000 |
                                                            min(x) < filter.mean-sanity.sd*filter.sd |
                                                            max(x) > filter.mean+sanity.sd*filter.sd
                                   ))
        }
        if(v == "surface_downwelling_longwave_flux_in_air"){ # SQRT
          # A NASA presentation has values topping out ~300 and min ~0:  https://ceres.larc.nasa.gov/documents/STM/2003-05/pdf/smith.pdf
          # A random journal article has 130 - 357.3: http://www.tandfonline.com/doi/full/10.1080/07055900.2012.760441
          # Based on what what CRUNCEP did, lets assume these are annual averages, so we can do 50% above it and for the min, in case we run tropics, lets go 130/4
          # ED2 sanity checks bound longwave at 40 & 600
          cols.redo <- which(apply(dat.pred, 2, function(x) min(x^2) < 40  | max(x^2) > 600 |
                                                            min(x^2) < filter.mean-sanity.sd*filter.sd |
                                                            max(x^2) > filter.mean+sanity.sd*filter.sd
                                     ))

        }
        if(v == "wind_speed"){
          # According to wikipedia, the hgihest wind speed ever recorded is a gust of 113 m/s; the maximum 5-mind wind speed is 49 m/s
          cols.redo <- which(apply(dat.pred, 2, function(x) max(x^2) > 85 |
                                                            min(x^2) < filter.mean-sanity.sd*filter.sd |
                                                            max(x^2) > filter.mean+sanity.sd*filter.sd
                                   ))
        }
        if(v == "precipitation_flux"){
          # According to wunderground, ~16" in 1 hr is the max
          # https://www.wunderground.com/blog/weatherhistorian/what-is-the-most-rain-to-ever-fall-in-one-minute-or-one-hour.html
          # 16; x25.4 = inches to mm; /(60*60) = hr to sec
          # Updated to ED2 max: 400 mm/hr
          cols.redo <- which(apply(dat.pred, 2, function(x) max(x) > 0.1111
                                   ))
        }

        n.new = length(cols.redo)
        if(force.sanity){
          sane.attempt = sane.attempt + 1
        } else {
          # If we're not forcing sanity, just stop now
          sane.attempt=sanity.tries + 1
        }
        # -----
      } # End while case

      # If we ran out of attempts, but want to foce sanity, do so now
      if(force.sanity & n.new>0){
        # If we're still struggling, but we have at least some workable columns, lets just duplicate those:
        if(n.new<n.ens){
          cols.safe <- 1:ncol(dat.pred)
          cols.safe <- cols.safe[!(cols.safe %in% cols.redo)]
          dat.pred[,cols.redo] <- dat.pred[,sample(cols.safe, n.new, replace=T)]
        } else {
          if(v=="surface_downwelling_shortwave_flux_in_air"){
            # Shouldn't be a huge problem, but it's not looking good
            # min(x) < 273.15-95 | max(x) > 273.15+70
            warning(paste("Forcing Sanity:", v))
            if(min(dat.pred) < max(filter.mean-sanity.sd*filter.sd)){
              qtrim <- max(filter.mean-sanity.sd*filter.sd)
              dat.pred[dat.pred < qtrim] <- qtrim
            }
            if(max(dat.pred) > min(1360, filter.mean+sanity.sd*filter.sd)){
              qtrim <- min(1360, filter.mean+sanity.sd*filter.sd)
              dat.pred[dat.pred > qtrim] <- qtrim
            }

          } else if(v=="air_temperature"){
            # Shouldn't be a huge problem, but it's not looking good
            # min(x) < 273.15-95 | max(x) > 273.15+70
            warning(paste("Forcing Sanity:", v))
            if(min(dat.pred) < max(273.15-95, filter.mean-sanity.sd*filter.sd )){
              qtrim <- max(273.15-95, filter.mean-sanity.sd*filter.sd)
              dat.pred[dat.pred < qtrim] <- qtrim
            }
            if(max(dat.pred) > min(273.15+70, filter.mean+sanity.sd*filter.sd)){
              qtrim <- min(273.15+70, filter.mean+sanity.sd*filter.sd)
              dat.pred[dat.pred > qtrim] <- qtrim
            }

          } else if(v=="air_pressure"){
            # A known problem child
            warning(paste("Forcing Sanity:", v))
            if(min(dat.pred) < max(870*100, filter.mean-sanity.sd*filter.sd )){
              qtrim <- max(870*100, filter.mean-sanity.sd*filter.sd)
              dat.pred[dat.pred < qtrim] <- qtrim
            }
            if(max(dat.pred) > min(1100*100, filter.mean+sanity.sd*filter.sd)){
              qtrim <- min(1100*100, filter.mean+sanity.sd*filter.sd)
              dat.pred[dat.pred > qtrim] <- qtrim
            }

          } else if(v=="surface_downwelling_longwave_flux_in_air"){
            # A known problem child
            # ED2 sanity checks boudn longwave at 40 & 600
            warning(paste("Forcing Sanity:", v))
            if(min(dat.pred^2) < max(40, filter.mean-sanity.sd*filter.sd )){
              qtrim <- max(40, filter.mean-sanity.sd*filter.sd)
              dat.pred[dat.pred^2 < qtrim] <- sqrt(qtrim)
            }
            if(max(dat.pred^2) > min(600, filter.mean+sanity.sd*filter.sd)){
              qtrim <- min(600, filter.mean+sanity.sd*filter.sd)
              dat.pred[dat.pred^2 > qtrim] <- sqrt(qtrim)
            }

          } else  if(v=="specific_humidity") {
            warning(paste("Forcing Sanity:", v))
            if(min(exp(dat.pred)) < max(1e-6, filter.mean-sanity.sd*filter.sd )){
              qtrim <- max(1e-6, filter.mean-sanity.sd*filter.sd)
              dat.pred[exp(dat.pred) < qtrim] <- log(qtrim)
            }
            if(max(exp(dat.pred)) > min(30e-3, filter.mean+sanity.sd*filter.sd)){
              qtrim <- min(40e-3, filter.mean+sanity.sd*filter.sd)
              dat.pred[exp(dat.pred) > qtrim] <- log(qtrim)
            }

          } else if(v=="wind_speed"){
            # A known problem child
            warning(paste("Forcing Sanity:", v))
            # if(min(dat.pred^2) < max(0, filter.mean-sanity.sd*filter.sd )){
            # qtrim <- max(0, 1)
            # dat.pred[dat.pred < qtrim] <- qtrim
            # }
            if(max(dat.pred^2) > min(50, filter.mean+sanity.sd*filter.sd)){
              qtrim <- min(50, filter.mean+sanity.sd*filter.sd)
              dat.pred[dat.pred^2 > qtrim] <- sqrt(qtrim)
            }

          } else {
            stop(paste("Unable to produce a sane prediction:", v, "- day", day.now, "; problem child =", paste(cols.redo, collapse=" ")))
          }

        }
      } # End force sanity

      ncdf4::nc_close(betas_nc)


      #----- Now we do a little quality control per variable
      # un-transforming our variables
      if (v %in% vars.sqrt) {
        dat.pred <- dat.pred^2
      } else if (v %in% vars.log) {
        dat.pred <- exp(dat.pred)
      }

      # ----------
      # Re-distribute precip so we don't get the constant drizzle problem
      # -- this could go earlier, but I'm being lazy because I don't want to mess with cols.redo
      # ----------
      if(v == "precipitation_flux"){
        # Pick the number of hours to spread rain across from our observed distribution
        # in case we don't have a large distribution, use multiple days
        if(day.now <=3) {
          rain.ind <- c(1:(day.now+3), (length(precip.distribution$hrs.rain)-3+day.now):length(precip.distribution$hrs.rain))
        } else if (day.now >= length(precip.distribution$hrs.rain)-3 ){
          rain.ind <-c(day.now:length(precip.distribution$hrs.rain), 1:(length(precip.distribution$hrs.rain)-day.now+3))
        } else {
          rain.ind <- (day.now-3):(day.now+3)
        }

        hrs.rain <- sample(unlist(precip.distribution$hrs.rain[rain.ind]),1)
        # hr.max <- sample(precip.distribution$hrs.max[[day.now]],1)

        for(j in 1:ncol(dat.pred)){
          obs.day <- nrow(dat.pred)/ncol(dat.pred)
          start.ind <- seq(1, nrow(dat.pred), by=obs.day)
          for(z in seq_along(start.ind)){
            rain.now <- dat.pred[start.ind[z]:(start.ind[z]+obs.day-1),j]
            hrs.now <- which(rain.now>0)

            if(length(hrs.now)<=hrs.rain) next # If we don't need to redistribute, skip what's next

            # Figure out when it's going to rain based on what normally has the most number of hours
            hrs.add <- sample(unlist(precip.distribution$hrs.max[rain.ind]), hrs.rain, replace=T)
            hrs.go <- hrs.now[!hrs.now %in% hrs.add]
            hrs.wet <- sample(hrs.add, length(hrs.go), replace=T)

            for(dry in seq_along(hrs.go)){
              rain.now[hrs.wet[dry]] <- rain.now[hrs.wet[dry]] + rain.now[hrs.go[dry]]
              rain.now[hrs.go[dry]] <- 0
            }

            # Put the rain back into place
            dat.pred[start.ind[z]:(start.ind[z]+obs.day-1),j] <- rain.now
          } # End row loop
        } # End column loop
      } # End hour redistribution
      # ----------

      # ----------
      # Begin propogating values and saving values Shortwave Radiaiton
      # ----------
      if (v == "surface_downwelling_shortwave_flux_in_air") {
        # Randomly pick which values to save & propogate

        if(ncol(dat.sim[[v]])>1){
          cols.prop <- as.integer(cols.list[i,])
          for (j in 1:ncol(dat.sim[[v]])) {
            dat.sim[[v]][rows.mod, j] <- dat.pred[dat.temp$ens == paste0("X", j), cols.prop[j]]
          }
        } else { # Only one ensemble member... it's really easy
          dat.sim[[v]][rows.mod, 1] <- dat.pred
        }


        dat.sim[[v]][rows.now[!rows.now %in% rows.mod], ] <- 0
      } else {

        if(ncol(dat.sim[[v]])>1){
          cols.prop <- as.integer(cols.list[i,])
          for (j in 1:ncol(dat.sim[[v]])) {
            dat.sim[[v]][rows.now, j] <- dat.pred[dat.temp$ens == paste0("X", j), cols.prop[j]]
          }
        } else { # Only one ensemble member... it's really easy
          dat.sim[[v]][rows.now, 1] <- dat.pred
        }
      }
      rm(mod.save)  # Clear out the model to save memory

      if(print.progress==TRUE){
        utils::setTxtProgressBar(pb, pb.index)
        pb.index <- pb.index + 1
      }

      rm(dat.temp, dat.pred)
    } # end day loop
    # --------------------------------

  }  # End vars.list
  # ---------- End of downscaling for loop
  return(dat.sim)
}
